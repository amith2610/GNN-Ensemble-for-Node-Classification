{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2480"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "adj = sp.load_npz('./data_2024/adj.npz')\n",
    "feat  = np.load('./data_2024/features.npy')\n",
    "labels = np.load('./data_2024/labels.npy')\n",
    "splits = json.load(open('./data_2024/splits.json'))\n",
    "idx_train, idx_test = splits['idx_train'], splits['idx_test']\n",
    "\n",
    "\n",
    "# Dimensionality Reduction\n",
    "n_components = 128\n",
    "pca = PCA(n_components=n_components)\n",
    "reduced_feat = pca.fit_transform(feat)\n",
    "\n",
    "\n",
    "# Converting the reduced features and other arrays to torch tensors\n",
    "reduced_feat = torch.tensor(reduced_feat, dtype=torch.float)\n",
    "full_labels = -1 * np.ones(shape=(reduced_feat.shape[0],), dtype=np.int64)\n",
    "full_labels[idx_train] = labels\n",
    "labels = torch.tensor(full_labels, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "edge_index, _ = from_scipy_sparse_matrix(adj)\n",
    "\n",
    "# Converting numpy arrays to torch tensors\n",
    "# feat = torch.tensor(feat, dtype=torch.float)\n",
    "# full_labels = -1 * np.ones(shape=(feat.shape[0],), dtype=np.int64)\n",
    "# full_labels[idx_train] = labels\n",
    "# labels = torch.tensor(full_labels, dtype=torch.long)\n",
    "\n",
    "data = Data(x=reduced_feat, edge_index=edge_index, y=labels)\n",
    "# data = Data(x=feat, edge_index=edge_index, y=labels)\n",
    "\n",
    "train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "train_mask[idx_train] = True\n",
    "test_mask[idx_test] = True\n",
    "data.train_mask = train_mask\n",
    "data.test_mask = test_mask\n",
    "\n",
    "\n",
    "# num_train = int(len(idx_train) * 0.85)\n",
    "\n",
    "# train_indices = idx_train[:num_train]\n",
    "# val_indices = idx_train[num_train:]\n",
    "\n",
    "# train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "# val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "\n",
    "# train_mask[train_indices] = True\n",
    "# val_mask[val_indices] = True\n",
    "\n",
    "# data.train_mask = train_mask\n",
    "# data.val_mask = val_mask\n",
    "\n",
    "idx_train =idx_train + idx_test\n",
    "\n",
    "train_mask[idx_test] = True\n",
    "data.train_mask = train_mask\n",
    "data.train_mask.tolist().count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2480, 128], edge_index=[2, 10100], y=[2480], train_mask=[2480], test_mask=[2480])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-1, 0, 1, 2, 3, 4, 5, 6}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data.y.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_hidden, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, num_hidden)\n",
    "        self.hid1 = GCNConv(num_hidden, 16)\n",
    "        self.hid2 = GCNConv(16, num_hidden)\n",
    "        self.conv2 = GCNConv(num_hidden, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.hid1(x, edge_index)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.hid2(x, edge_index)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/8\n",
      "Epoch 0: Train Loss: 1.950131893157959, Val Loss: 1.9227735996246338, Val Acc: 0.2903\n",
      "Epoch 10: Train Loss: 0.600151777267456, Val Loss: 0.7727506160736084, Val Acc: 0.8387\n",
      "Epoch 20: Train Loss: 0.14646536111831665, Val Loss: 1.2399567365646362, Val Acc: 0.8226\n",
      "Epoch 30: Train Loss: 0.10245935618877411, Val Loss: 1.361914038658142, Val Acc: 0.8387\n",
      "Epoch 40: Train Loss: 0.08514180034399033, Val Loss: 1.0897802114486694, Val Acc: 0.7903\n",
      "Epoch 50: Train Loss: 0.07256221771240234, Val Loss: 1.0307186841964722, Val Acc: 0.8226\n",
      "Epoch 60: Train Loss: 0.06738454103469849, Val Loss: 0.7299565076828003, Val Acc: 0.8226\n",
      "Epoch 70: Train Loss: 0.058953724801540375, Val Loss: 0.8528133630752563, Val Acc: 0.8065\n",
      "Epoch 80: Train Loss: 0.05527227371931076, Val Loss: 0.8474425077438354, Val Acc: 0.7903\n",
      "Epoch 90: Train Loss: 0.04216095805168152, Val Loss: 0.9238696098327637, Val Acc: 0.7742\n",
      "Epoch 100: Train Loss: 0.04192211106419563, Val Loss: 0.8398582935333252, Val Acc: 0.7903\n",
      "Epoch 110: Train Loss: 0.03959176316857338, Val Loss: 0.8058508634567261, Val Acc: 0.7742\n",
      "Epoch 120: Train Loss: 0.03479691594839096, Val Loss: 1.0436369180679321, Val Acc: 0.7903\n",
      "Epoch 130: Train Loss: 0.0447862334549427, Val Loss: 0.8835268616676331, Val Acc: 0.7903\n",
      "Epoch 140: Train Loss: 0.03031536377966404, Val Loss: 1.2632182836532593, Val Acc: 0.8226\n",
      "Epoch 150: Train Loss: 0.045273978263139725, Val Loss: 1.1776106357574463, Val Acc: 0.7903\n",
      "Epoch 160: Train Loss: 0.033173009753227234, Val Loss: 1.3307440280914307, Val Acc: 0.8065\n",
      "Epoch 170: Train Loss: 0.06068466231226921, Val Loss: 1.1527467966079712, Val Acc: 0.8226\n",
      "Epoch 180: Train Loss: 0.056298840790987015, Val Loss: 1.0209460258483887, Val Acc: 0.7903\n",
      "Epoch 190: Train Loss: 0.05076433718204498, Val Loss: 0.9494652152061462, Val Acc: 0.7742\n",
      "Fold 2/8\n",
      "Epoch 0: Train Loss: 1.955645203590393, Val Loss: 1.954272747039795, Val Acc: 0.3065\n",
      "Epoch 10: Train Loss: 0.49079999327659607, Val Loss: 1.285345435142517, Val Acc: 0.7903\n",
      "Epoch 20: Train Loss: 0.18967562913894653, Val Loss: 2.2115345001220703, Val Acc: 0.7742\n",
      "Epoch 30: Train Loss: 0.08026200532913208, Val Loss: 2.0681099891662598, Val Acc: 0.7742\n",
      "Epoch 40: Train Loss: 0.041863612830638885, Val Loss: 1.6857465505599976, Val Acc: 0.7742\n",
      "Epoch 50: Train Loss: 0.032253775745630264, Val Loss: 1.4184077978134155, Val Acc: 0.7903\n",
      "Epoch 60: Train Loss: 0.04777504503726959, Val Loss: 1.47171151638031, Val Acc: 0.7742\n",
      "Epoch 70: Train Loss: 0.05299201235175133, Val Loss: 1.8507611751556396, Val Acc: 0.8065\n",
      "Epoch 80: Train Loss: 0.03539950028061867, Val Loss: 1.7215675115585327, Val Acc: 0.7742\n",
      "Epoch 90: Train Loss: 0.025531025603413582, Val Loss: 1.853024959564209, Val Acc: 0.8226\n",
      "Epoch 100: Train Loss: 0.07410403341054916, Val Loss: 2.0838115215301514, Val Acc: 0.7742\n",
      "Epoch 110: Train Loss: 0.041109804064035416, Val Loss: 1.6389023065567017, Val Acc: 0.8226\n",
      "Epoch 120: Train Loss: 0.03840026259422302, Val Loss: 1.7353169918060303, Val Acc: 0.8065\n",
      "Epoch 130: Train Loss: 0.033315740525722504, Val Loss: 1.4863463640213013, Val Acc: 0.8065\n",
      "Epoch 140: Train Loss: 0.027014730498194695, Val Loss: 1.774343729019165, Val Acc: 0.7903\n",
      "Epoch 150: Train Loss: 0.027058016508817673, Val Loss: 1.9299168586730957, Val Acc: 0.8065\n",
      "Epoch 160: Train Loss: 0.031981587409973145, Val Loss: 2.2129693031311035, Val Acc: 0.8065\n",
      "Epoch 170: Train Loss: 0.03584612160921097, Val Loss: 2.191784143447876, Val Acc: 0.7903\n",
      "Epoch 180: Train Loss: 0.02108846977353096, Val Loss: 2.7269206047058105, Val Acc: 0.6935\n",
      "Epoch 190: Train Loss: 0.058463625609874725, Val Loss: 2.3171470165252686, Val Acc: 0.7581\n",
      "Fold 3/8\n",
      "Epoch 0: Train Loss: 1.939064383506775, Val Loss: 1.9311540126800537, Val Acc: 0.2903\n",
      "Epoch 10: Train Loss: 0.4537251889705658, Val Loss: 0.8706258535385132, Val Acc: 0.8226\n",
      "Epoch 20: Train Loss: 0.26000550389289856, Val Loss: 1.6571707725524902, Val Acc: 0.8226\n",
      "Epoch 30: Train Loss: 0.1239774227142334, Val Loss: 2.1804590225219727, Val Acc: 0.7903\n",
      "Epoch 40: Train Loss: 0.06016169488430023, Val Loss: 1.8182936906814575, Val Acc: 0.7903\n",
      "Epoch 50: Train Loss: 0.11386994272470474, Val Loss: 1.3162823915481567, Val Acc: 0.8387\n",
      "Epoch 60: Train Loss: 0.05802391842007637, Val Loss: 2.229902982711792, Val Acc: 0.8065\n",
      "Epoch 70: Train Loss: 0.057534266263246536, Val Loss: 1.3557530641555786, Val Acc: 0.8548\n",
      "Epoch 80: Train Loss: 0.04726283252239227, Val Loss: 1.1737488508224487, Val Acc: 0.8065\n",
      "Epoch 90: Train Loss: 0.039825886487960815, Val Loss: 1.403921365737915, Val Acc: 0.8226\n",
      "Epoch 100: Train Loss: 0.044941872358322144, Val Loss: 1.2685943841934204, Val Acc: 0.8387\n",
      "Epoch 110: Train Loss: 0.03167515993118286, Val Loss: 1.3049275875091553, Val Acc: 0.8548\n",
      "Epoch 120: Train Loss: 0.036365050822496414, Val Loss: 1.8302531242370605, Val Acc: 0.8065\n",
      "Epoch 130: Train Loss: 0.04757886379957199, Val Loss: 1.4386569261550903, Val Acc: 0.8065\n",
      "Epoch 140: Train Loss: 0.029081176966428757, Val Loss: 1.4422459602355957, Val Acc: 0.8226\n",
      "Epoch 150: Train Loss: 0.0726512223482132, Val Loss: 1.2891340255737305, Val Acc: 0.7903\n",
      "Epoch 160: Train Loss: 0.03649258613586426, Val Loss: 0.8951860070228577, Val Acc: 0.8387\n",
      "Epoch 170: Train Loss: 0.03165769949555397, Val Loss: 1.28908109664917, Val Acc: 0.8548\n",
      "Epoch 180: Train Loss: 0.052083030343055725, Val Loss: 1.2513954639434814, Val Acc: 0.8226\n",
      "Epoch 190: Train Loss: 0.03713446110486984, Val Loss: 1.2174601554870605, Val Acc: 0.7581\n",
      "Fold 4/8\n",
      "Epoch 0: Train Loss: 1.9286963939666748, Val Loss: 1.9329197406768799, Val Acc: 0.3065\n",
      "Epoch 10: Train Loss: 0.4084073007106781, Val Loss: 1.603303074836731, Val Acc: 0.8387\n",
      "Epoch 20: Train Loss: 0.22662878036499023, Val Loss: 1.753397822380066, Val Acc: 0.8387\n",
      "Epoch 30: Train Loss: 0.1507224440574646, Val Loss: 2.1060495376586914, Val Acc: 0.8387\n",
      "Epoch 40: Train Loss: 0.17829960584640503, Val Loss: 1.4273205995559692, Val Acc: 0.8548\n",
      "Epoch 50: Train Loss: 0.08824475109577179, Val Loss: 1.8347883224487305, Val Acc: 0.8065\n",
      "Epoch 60: Train Loss: 0.06080292537808418, Val Loss: 1.7283942699432373, Val Acc: 0.8226\n",
      "Epoch 70: Train Loss: 0.06330060213804245, Val Loss: 1.4366172552108765, Val Acc: 0.8548\n",
      "Epoch 80: Train Loss: 0.03516034036874771, Val Loss: 1.3024364709854126, Val Acc: 0.8387\n",
      "Epoch 90: Train Loss: 0.02829177863895893, Val Loss: 1.619528889656067, Val Acc: 0.8387\n",
      "Epoch 100: Train Loss: 0.03977076709270477, Val Loss: 1.166976809501648, Val Acc: 0.8548\n",
      "Epoch 110: Train Loss: 0.040554556995630264, Val Loss: 1.3272981643676758, Val Acc: 0.8226\n",
      "Epoch 120: Train Loss: 0.040626104921102524, Val Loss: 1.3303369283676147, Val Acc: 0.8387\n",
      "Epoch 130: Train Loss: 0.041957613080739975, Val Loss: 1.2361394166946411, Val Acc: 0.8548\n",
      "Epoch 140: Train Loss: 0.056594669818878174, Val Loss: 1.777881383895874, Val Acc: 0.7903\n",
      "Epoch 150: Train Loss: 0.05723804235458374, Val Loss: 1.475816249847412, Val Acc: 0.8387\n",
      "Epoch 160: Train Loss: 0.037292879074811935, Val Loss: 1.7879502773284912, Val Acc: 0.8387\n",
      "Epoch 170: Train Loss: 0.052169181406497955, Val Loss: 1.3572689294815063, Val Acc: 0.8226\n",
      "Epoch 180: Train Loss: 0.04261943697929382, Val Loss: 1.1328526735305786, Val Acc: 0.8871\n",
      "Epoch 190: Train Loss: 0.038162991404533386, Val Loss: 1.2791539430618286, Val Acc: 0.8226\n",
      "Fold 5/8\n",
      "Epoch 0: Train Loss: 1.9600245952606201, Val Loss: 1.9510518312454224, Val Acc: 0.3871\n",
      "Epoch 10: Train Loss: 0.37414875626564026, Val Loss: 0.9591190814971924, Val Acc: 0.8548\n",
      "Epoch 20: Train Loss: 0.21443231403827667, Val Loss: 1.9183095693588257, Val Acc: 0.7903\n",
      "Epoch 30: Train Loss: 0.07659434527158737, Val Loss: 2.6889302730560303, Val Acc: 0.8226\n",
      "Epoch 40: Train Loss: 0.06050688400864601, Val Loss: 2.238377332687378, Val Acc: 0.8548\n",
      "Epoch 50: Train Loss: 0.06505735218524933, Val Loss: 1.3328605890274048, Val Acc: 0.8387\n",
      "Epoch 60: Train Loss: 0.05511202663183212, Val Loss: 1.330055594444275, Val Acc: 0.8387\n",
      "Epoch 70: Train Loss: 0.04914093762636185, Val Loss: 1.3009389638900757, Val Acc: 0.8548\n",
      "Epoch 80: Train Loss: 0.046416912227869034, Val Loss: 1.6459240913391113, Val Acc: 0.8548\n",
      "Epoch 90: Train Loss: 0.051676008850336075, Val Loss: 1.6372393369674683, Val Acc: 0.8548\n",
      "Epoch 100: Train Loss: 0.05670500546693802, Val Loss: 1.8736711740493774, Val Acc: 0.8065\n",
      "Epoch 110: Train Loss: 0.03602593392133713, Val Loss: 1.8938591480255127, Val Acc: 0.8226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120: Train Loss: 0.052247676998376846, Val Loss: 1.503338098526001, Val Acc: 0.8065\n",
      "Epoch 130: Train Loss: 0.05362817272543907, Val Loss: 1.3479056358337402, Val Acc: 0.8387\n",
      "Epoch 140: Train Loss: 0.025180209428071976, Val Loss: 1.6516815423965454, Val Acc: 0.8387\n",
      "Epoch 150: Train Loss: 0.040628205984830856, Val Loss: 1.4804039001464844, Val Acc: 0.8065\n",
      "Epoch 160: Train Loss: 0.04629680514335632, Val Loss: 1.7349607944488525, Val Acc: 0.7742\n",
      "Epoch 170: Train Loss: 0.043682731688022614, Val Loss: 1.762916088104248, Val Acc: 0.8226\n",
      "Epoch 180: Train Loss: 0.035872966051101685, Val Loss: 1.8489669561386108, Val Acc: 0.8226\n",
      "Epoch 190: Train Loss: 0.03677970543503761, Val Loss: 1.8239011764526367, Val Acc: 0.8387\n",
      "Fold 6/8\n",
      "Epoch 0: Train Loss: 1.9505119323730469, Val Loss: 1.946790337562561, Val Acc: 0.3710\n",
      "Epoch 10: Train Loss: 0.40146687626838684, Val Loss: 1.4006539583206177, Val Acc: 0.8226\n",
      "Epoch 20: Train Loss: 0.1241835281252861, Val Loss: 2.6854312419891357, Val Acc: 0.8226\n",
      "Epoch 30: Train Loss: 0.08159184455871582, Val Loss: 2.6848208904266357, Val Acc: 0.8226\n",
      "Epoch 40: Train Loss: 0.051076121628284454, Val Loss: 2.090017080307007, Val Acc: 0.8548\n",
      "Epoch 50: Train Loss: 0.05840687453746796, Val Loss: 2.327550172805786, Val Acc: 0.8226\n",
      "Epoch 60: Train Loss: 0.04266638681292534, Val Loss: 2.2084238529205322, Val Acc: 0.8387\n",
      "Epoch 70: Train Loss: 0.047209978103637695, Val Loss: 1.2838410139083862, Val Acc: 0.8871\n",
      "Epoch 80: Train Loss: 0.044169776141643524, Val Loss: 2.06803560256958, Val Acc: 0.8548\n",
      "Epoch 90: Train Loss: 0.04149797931313515, Val Loss: 1.6529051065444946, Val Acc: 0.8387\n",
      "Epoch 100: Train Loss: 0.04161081835627556, Val Loss: 1.8459489345550537, Val Acc: 0.8548\n",
      "Epoch 110: Train Loss: 0.03345448523759842, Val Loss: 1.0714876651763916, Val Acc: 0.8387\n",
      "Epoch 120: Train Loss: 0.040262024849653244, Val Loss: 1.3602149486541748, Val Acc: 0.8548\n",
      "Epoch 130: Train Loss: 0.029089519754052162, Val Loss: 1.4839680194854736, Val Acc: 0.8548\n",
      "Epoch 140: Train Loss: 0.039795711636543274, Val Loss: 1.7444268465042114, Val Acc: 0.8548\n",
      "Epoch 150: Train Loss: 0.03935801237821579, Val Loss: 2.340580701828003, Val Acc: 0.8548\n",
      "Epoch 160: Train Loss: 0.04512160271406174, Val Loss: 1.9223856925964355, Val Acc: 0.8548\n",
      "Epoch 170: Train Loss: 0.0457540825009346, Val Loss: 2.2482035160064697, Val Acc: 0.8548\n",
      "Epoch 180: Train Loss: 0.05868332087993622, Val Loss: 2.315962314605713, Val Acc: 0.8065\n",
      "Epoch 190: Train Loss: 0.04838468134403229, Val Loss: 1.7846524715423584, Val Acc: 0.8065\n",
      "Fold 7/8\n",
      "Epoch 0: Train Loss: 1.9313584566116333, Val Loss: 1.9384623765945435, Val Acc: 0.5484\n",
      "Epoch 10: Train Loss: 0.5806226134300232, Val Loss: 1.9621907472610474, Val Acc: 0.7581\n",
      "Epoch 20: Train Loss: 0.14092396199703217, Val Loss: 1.6927270889282227, Val Acc: 0.7581\n",
      "Epoch 30: Train Loss: 0.07303151488304138, Val Loss: 1.8814417123794556, Val Acc: 0.6935\n",
      "Epoch 40: Train Loss: 0.059767112135887146, Val Loss: 1.7447928190231323, Val Acc: 0.7419\n",
      "Epoch 50: Train Loss: 0.06159592419862747, Val Loss: 1.6312395334243774, Val Acc: 0.7903\n",
      "Epoch 60: Train Loss: 0.0544280931353569, Val Loss: 1.6640808582305908, Val Acc: 0.7742\n",
      "Epoch 70: Train Loss: 0.038733452558517456, Val Loss: 1.775763988494873, Val Acc: 0.7742\n",
      "Epoch 80: Train Loss: 0.052616920322179794, Val Loss: 1.7163920402526855, Val Acc: 0.7581\n",
      "Epoch 90: Train Loss: 0.04879464954137802, Val Loss: 2.063755750656128, Val Acc: 0.7419\n",
      "Epoch 100: Train Loss: 0.030038317665457726, Val Loss: 2.1919455528259277, Val Acc: 0.7903\n",
      "Epoch 110: Train Loss: 0.03818595036864281, Val Loss: 2.1003637313842773, Val Acc: 0.7258\n",
      "Epoch 120: Train Loss: 0.046750981360673904, Val Loss: 2.07228422164917, Val Acc: 0.7581\n",
      "Epoch 130: Train Loss: 0.032210856676101685, Val Loss: 2.119070053100586, Val Acc: 0.7581\n",
      "Epoch 140: Train Loss: 0.03790067136287689, Val Loss: 2.0480642318725586, Val Acc: 0.7419\n",
      "Epoch 150: Train Loss: 0.038566190749406815, Val Loss: 2.8650689125061035, Val Acc: 0.7258\n",
      "Epoch 160: Train Loss: 0.038464829325675964, Val Loss: 1.925727367401123, Val Acc: 0.7258\n",
      "Epoch 170: Train Loss: 0.050087396055459976, Val Loss: 1.7839854955673218, Val Acc: 0.7258\n",
      "Epoch 180: Train Loss: 0.03439070284366608, Val Loss: 1.7650126218795776, Val Acc: 0.7742\n",
      "Epoch 190: Train Loss: 0.03535773232579231, Val Loss: 2.04017972946167, Val Acc: 0.7742\n",
      "Fold 8/8\n",
      "Epoch 0: Train Loss: 1.950055718421936, Val Loss: 1.958634853363037, Val Acc: 0.4032\n",
      "Epoch 10: Train Loss: 0.4360254108905792, Val Loss: 1.1844656467437744, Val Acc: 0.8548\n",
      "Epoch 20: Train Loss: 0.18207204341888428, Val Loss: 1.0369621515274048, Val Acc: 0.8548\n",
      "Epoch 30: Train Loss: 0.07802674919366837, Val Loss: 0.5690943598747253, Val Acc: 0.8226\n",
      "Epoch 40: Train Loss: 0.07929342240095139, Val Loss: 1.5615625381469727, Val Acc: 0.8065\n",
      "Epoch 50: Train Loss: 0.06525249034166336, Val Loss: 1.1112865209579468, Val Acc: 0.8226\n",
      "Epoch 60: Train Loss: 0.06635435670614243, Val Loss: 1.240869164466858, Val Acc: 0.8387\n",
      "Epoch 70: Train Loss: 0.060693591833114624, Val Loss: 1.0488271713256836, Val Acc: 0.8548\n",
      "Epoch 80: Train Loss: 0.05941960588097572, Val Loss: 1.3475350141525269, Val Acc: 0.7903\n",
      "Epoch 90: Train Loss: 0.0663469061255455, Val Loss: 1.547631859779358, Val Acc: 0.8387\n",
      "Epoch 100: Train Loss: 0.056731101125478745, Val Loss: 1.189956784248352, Val Acc: 0.8226\n",
      "Epoch 110: Train Loss: 0.0451975092291832, Val Loss: 1.3133141994476318, Val Acc: 0.8387\n",
      "Epoch 120: Train Loss: 0.06727466732263565, Val Loss: 1.549028992652893, Val Acc: 0.8065\n",
      "Epoch 130: Train Loss: 0.08172984421253204, Val Loss: 2.3664937019348145, Val Acc: 0.8065\n",
      "Epoch 140: Train Loss: 0.08926831930875778, Val Loss: 1.77033269405365, Val Acc: 0.7742\n",
      "Epoch 150: Train Loss: 0.043175045400857925, Val Loss: 1.8960269689559937, Val Acc: 0.7903\n",
      "Epoch 160: Train Loss: 0.056444957852363586, Val Loss: 1.5212184190750122, Val Acc: 0.8065\n",
      "Epoch 170: Train Loss: 0.04945904389023781, Val Loss: 1.3910653591156006, Val Acc: 0.8226\n",
      "Epoch 180: Train Loss: 0.052506379783153534, Val Loss: 1.080270528793335, Val Acc: 0.7903\n",
      "Epoch 190: Train Loss: 0.051094673573970795, Val Loss: 1.0532429218292236, Val Acc: 0.8226\n",
      "Average Validation Accuracy: 0.8677\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "k = 8\n",
    "kf = StratifiedKFold(n_splits=k)\n",
    "idx_train_np = np.array(idx_train)\n",
    "labels = data.y.numpy()[idx_train_np]\n",
    "\n",
    "best_accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(idx_train_np, labels)):\n",
    "    print(f\"Fold {fold+1}/{k}\")\n",
    "\n",
    "    model = GCN(num_node_features=data.x.shape[1], \n",
    "                num_hidden=64,\n",
    "                num_classes=(data.y.max()+1).item()\n",
    "               ).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.09, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    \n",
    "    \n",
    "    data.train_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    data.val_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    data.train_mask[idx_train_np[train_idx]] = True\n",
    "    data.val_mask[idx_train_np[val_idx]] = True\n",
    "    \n",
    "    best_val_acc = 0 \n",
    "    best_model_state = None \n",
    "    \n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "#         loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask], ignore_index=-1)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                pred = model(data).argmax(dim=1)\n",
    "#                 correct = (pred[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "#                 acc = int(correct) / int(data.val_mask.sum())\n",
    "                \n",
    "                valid_labels_mask = data.y[data.val_mask] != -1  # Mask to select valid labels not equal to -1\n",
    "                correct_predictions = pred[data.val_mask][valid_labels_mask] == data.y[data.val_mask][valid_labels_mask]\n",
    "                correct = correct_predictions.sum()\n",
    "                acc = int(correct) / int(valid_labels_mask.sum())\n",
    "                \n",
    "                if acc > best_val_acc and acc >= 0.84:\n",
    "                    best_val_acc = acc\n",
    "                    best_model_state = model.state_dict()\n",
    "                    \n",
    "                val_loss = F.nll_loss(out[data.val_mask], data.y[data.val_mask], ignore_index=-1)\n",
    "                print(f'Epoch {epoch}: Train Loss: {loss.item()}, Val Loss: {val_loss.item()}, Val Acc: {acc:.4f}')\n",
    "            \n",
    "    if best_model_state is not None:\n",
    "        torch.save(best_model_state, f'gcn_best_fold_{fold+1}.pt')\n",
    "    if best_val_acc>0:    \n",
    "        best_accuracies.append(best_val_acc)\n",
    "    \n",
    "average_accuracy = sum(best_accuracies) / len(best_accuracies)\n",
    "print(f'Average Validation Accuracy: {average_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPNP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/9\n",
      "Epoch 0: Train Loss: 1.9440314769744873, Val Loss: 1.9476426839828491, Val Acc: 0.2857\n",
      "Epoch 20: Train Loss: 0.8606126308441162, Val Loss: 0.9268234372138977, Val Acc: 0.8036\n",
      "Epoch 40: Train Loss: 0.321504145860672, Val Loss: 0.4543730318546295, Val Acc: 0.8929\n",
      "Epoch 60: Train Loss: 0.15981808304786682, Val Loss: 0.4407646358013153, Val Acc: 0.8750\n",
      "Epoch 80: Train Loss: 0.11944343894720078, Val Loss: 0.45061662793159485, Val Acc: 0.8571\n",
      "Epoch 100: Train Loss: 0.09612695872783661, Val Loss: 0.39936086535453796, Val Acc: 0.8393\n",
      "Epoch 120: Train Loss: 0.08140906691551208, Val Loss: 0.4857498109340668, Val Acc: 0.8393\n",
      "Epoch 140: Train Loss: 0.08108352869749069, Val Loss: 0.44900423288345337, Val Acc: 0.8393\n",
      "Epoch 160: Train Loss: 0.06982582807540894, Val Loss: 0.5949245095252991, Val Acc: 0.8393\n",
      "Epoch 180: Train Loss: 0.06191379949450493, Val Loss: 0.4607880413532257, Val Acc: 0.8214\n",
      "Fold 2/9\n",
      "Epoch 0: Train Loss: 1.959958553314209, Val Loss: 1.9598253965377808, Val Acc: 0.3273\n",
      "Epoch 20: Train Loss: 0.8693767189979553, Val Loss: 1.0325084924697876, Val Acc: 0.6364\n",
      "Epoch 40: Train Loss: 0.3151727020740509, Val Loss: 0.5965242385864258, Val Acc: 0.8727\n",
      "Epoch 60: Train Loss: 0.17390143871307373, Val Loss: 0.5424538254737854, Val Acc: 0.8545\n",
      "Epoch 80: Train Loss: 0.12309544533491135, Val Loss: 0.49344274401664734, Val Acc: 0.8182\n",
      "Epoch 100: Train Loss: 0.09751515835523605, Val Loss: 0.534731924533844, Val Acc: 0.8364\n",
      "Epoch 120: Train Loss: 0.08100448548793793, Val Loss: 0.5897139310836792, Val Acc: 0.8364\n",
      "Epoch 140: Train Loss: 0.08722838759422302, Val Loss: 0.6831482648849487, Val Acc: 0.8182\n",
      "Epoch 160: Train Loss: 0.0759631097316742, Val Loss: 0.726081371307373, Val Acc: 0.8182\n",
      "Epoch 180: Train Loss: 0.06944037228822708, Val Loss: 0.555453896522522, Val Acc: 0.8000\n",
      "Fold 3/9\n",
      "Epoch 0: Train Loss: 1.9533518552780151, Val Loss: 1.9496774673461914, Val Acc: 0.4000\n",
      "Epoch 20: Train Loss: 0.8664979934692383, Val Loss: 0.9331661462783813, Val Acc: 0.7455\n",
      "Epoch 40: Train Loss: 0.30313581228256226, Val Loss: 0.5189065337181091, Val Acc: 0.8727\n",
      "Epoch 60: Train Loss: 0.16391493380069733, Val Loss: 0.4890269339084625, Val Acc: 0.8727\n",
      "Epoch 80: Train Loss: 0.11680909246206284, Val Loss: 0.6218299269676208, Val Acc: 0.8545\n",
      "Epoch 100: Train Loss: 0.10069319605827332, Val Loss: 0.48651957511901855, Val Acc: 0.8364\n",
      "Epoch 120: Train Loss: 0.08886108547449112, Val Loss: 0.48331522941589355, Val Acc: 0.8545\n",
      "Epoch 140: Train Loss: 0.07819528132677078, Val Loss: 0.45782530307769775, Val Acc: 0.8909\n",
      "Epoch 160: Train Loss: 0.08367732912302017, Val Loss: 0.6022164821624756, Val Acc: 0.8909\n",
      "Epoch 180: Train Loss: 0.07396219670772552, Val Loss: 0.5761406421661377, Val Acc: 0.8727\n",
      "Fold 4/9\n",
      "Epoch 0: Train Loss: 1.9424346685409546, Val Loss: 1.943420648574829, Val Acc: 0.3818\n",
      "Epoch 20: Train Loss: 0.8544798493385315, Val Loss: 0.9388888478279114, Val Acc: 0.7818\n",
      "Epoch 40: Train Loss: 0.3289661109447479, Val Loss: 0.41735467314720154, Val Acc: 0.8727\n",
      "Epoch 60: Train Loss: 0.17439821362495422, Val Loss: 0.36194756627082825, Val Acc: 0.8909\n",
      "Epoch 80: Train Loss: 0.12433579564094543, Val Loss: 0.34971868991851807, Val Acc: 0.8545\n",
      "Epoch 100: Train Loss: 0.09600421786308289, Val Loss: 0.35729697346687317, Val Acc: 0.8545\n",
      "Epoch 120: Train Loss: 0.09070377796888351, Val Loss: 0.3589230477809906, Val Acc: 0.8545\n",
      "Epoch 140: Train Loss: 0.0753435343503952, Val Loss: 0.40412038564682007, Val Acc: 0.8545\n",
      "Epoch 160: Train Loss: 0.0729643777012825, Val Loss: 0.3999517560005188, Val Acc: 0.8364\n",
      "Epoch 180: Train Loss: 0.0787324607372284, Val Loss: 0.44528937339782715, Val Acc: 0.8364\n",
      "Fold 5/9\n",
      "Epoch 0: Train Loss: 1.9537755250930786, Val Loss: 1.949009895324707, Val Acc: 0.3091\n",
      "Epoch 20: Train Loss: 0.8498291373252869, Val Loss: 0.8608822822570801, Val Acc: 0.8182\n",
      "Epoch 40: Train Loss: 0.29877597093582153, Val Loss: 0.4236167073249817, Val Acc: 0.9273\n",
      "Epoch 60: Train Loss: 0.17063692212104797, Val Loss: 0.4501575529575348, Val Acc: 0.9273\n",
      "Epoch 80: Train Loss: 0.11753661185503006, Val Loss: 0.4574510455131531, Val Acc: 0.8545\n",
      "Epoch 100: Train Loss: 0.08812768012285233, Val Loss: 0.379284143447876, Val Acc: 0.8909\n",
      "Epoch 120: Train Loss: 0.09249065816402435, Val Loss: 0.48238351941108704, Val Acc: 0.8727\n",
      "Epoch 140: Train Loss: 0.07425028830766678, Val Loss: 0.46842172741889954, Val Acc: 0.8727\n",
      "Epoch 160: Train Loss: 0.07218974083662033, Val Loss: 0.559578537940979, Val Acc: 0.8545\n",
      "Epoch 180: Train Loss: 0.07322187721729279, Val Loss: 0.5106978416442871, Val Acc: 0.8545\n",
      "Fold 6/9\n",
      "Epoch 0: Train Loss: 1.9510592222213745, Val Loss: 1.9458212852478027, Val Acc: 0.1818\n",
      "Epoch 20: Train Loss: 0.8130584359169006, Val Loss: 0.907763659954071, Val Acc: 0.7636\n",
      "Epoch 40: Train Loss: 0.28818753361701965, Val Loss: 0.49412745237350464, Val Acc: 0.8727\n",
      "Epoch 60: Train Loss: 0.16247783601284027, Val Loss: 0.5026273727416992, Val Acc: 0.8364\n",
      "Epoch 80: Train Loss: 0.11210016161203384, Val Loss: 0.5570186376571655, Val Acc: 0.8545\n",
      "Epoch 100: Train Loss: 0.1057923212647438, Val Loss: 0.593104362487793, Val Acc: 0.8545\n",
      "Epoch 120: Train Loss: 0.09395821392536163, Val Loss: 0.548443615436554, Val Acc: 0.8545\n",
      "Epoch 140: Train Loss: 0.07846542447805405, Val Loss: 0.6299297213554382, Val Acc: 0.8364\n",
      "Epoch 160: Train Loss: 0.0727953389286995, Val Loss: 0.5875352621078491, Val Acc: 0.8545\n",
      "Epoch 180: Train Loss: 0.07532629370689392, Val Loss: 0.6067401766777039, Val Acc: 0.8545\n",
      "Fold 7/9\n",
      "Epoch 0: Train Loss: 1.9392294883728027, Val Loss: 1.9431205987930298, Val Acc: 0.2727\n",
      "Epoch 20: Train Loss: 0.8435444831848145, Val Loss: 1.11821711063385, Val Acc: 0.7818\n",
      "Epoch 40: Train Loss: 0.2849107086658478, Val Loss: 0.8121784329414368, Val Acc: 0.8182\n",
      "Epoch 60: Train Loss: 0.14629776775836945, Val Loss: 0.9354447722434998, Val Acc: 0.8364\n",
      "Epoch 80: Train Loss: 0.12018687278032303, Val Loss: 1.0400989055633545, Val Acc: 0.8364\n",
      "Epoch 100: Train Loss: 0.08620744943618774, Val Loss: 1.0108857154846191, Val Acc: 0.8182\n",
      "Epoch 120: Train Loss: 0.08517922461032867, Val Loss: 1.1851493120193481, Val Acc: 0.8182\n",
      "Epoch 140: Train Loss: 0.06964202225208282, Val Loss: 1.0190706253051758, Val Acc: 0.8000\n",
      "Epoch 160: Train Loss: 0.06975149363279343, Val Loss: 1.150167465209961, Val Acc: 0.8000\n",
      "Epoch 180: Train Loss: 0.0619967058300972, Val Loss: 1.1498395204544067, Val Acc: 0.8000\n",
      "Fold 8/9\n",
      "Epoch 0: Train Loss: 1.9498435258865356, Val Loss: 1.9477808475494385, Val Acc: 0.1636\n",
      "Epoch 20: Train Loss: 0.8650240302085876, Val Loss: 0.9110706448554993, Val Acc: 0.7273\n",
      "Epoch 40: Train Loss: 0.3449855446815491, Val Loss: 0.4262959957122803, Val Acc: 0.8727\n",
      "Epoch 60: Train Loss: 0.17409507930278778, Val Loss: 0.42000555992126465, Val Acc: 0.8727\n",
      "Epoch 80: Train Loss: 0.12101717293262482, Val Loss: 0.4507293105125427, Val Acc: 0.8727\n",
      "Epoch 100: Train Loss: 0.10701735317707062, Val Loss: 0.47458988428115845, Val Acc: 0.8545\n",
      "Epoch 120: Train Loss: 0.08646801114082336, Val Loss: 0.486125111579895, Val Acc: 0.8545\n",
      "Epoch 140: Train Loss: 0.07789915800094604, Val Loss: 0.5451732873916626, Val Acc: 0.8727\n",
      "Epoch 160: Train Loss: 0.06496932357549667, Val Loss: 0.5408169031143188, Val Acc: 0.8727\n",
      "Epoch 180: Train Loss: 0.07671015709638596, Val Loss: 0.4107816517353058, Val Acc: 0.8727\n",
      "Fold 9/9\n",
      "Epoch 0: Train Loss: 1.9448566436767578, Val Loss: 1.946136236190796, Val Acc: 0.2909\n",
      "Epoch 20: Train Loss: 0.86855548620224, Val Loss: 0.9766281247138977, Val Acc: 0.6727\n",
      "Epoch 40: Train Loss: 0.3160744309425354, Val Loss: 0.5403907895088196, Val Acc: 0.8727\n",
      "Epoch 60: Train Loss: 0.15650445222854614, Val Loss: 0.5007976293563843, Val Acc: 0.8364\n",
      "Epoch 80: Train Loss: 0.11668659746646881, Val Loss: 0.5497011542320251, Val Acc: 0.8364\n",
      "Epoch 100: Train Loss: 0.10626929998397827, Val Loss: 0.533491313457489, Val Acc: 0.8364\n",
      "Epoch 120: Train Loss: 0.08426970988512039, Val Loss: 0.5468328595161438, Val Acc: 0.8364\n",
      "Epoch 140: Train Loss: 0.08197159320116043, Val Loss: 0.5795043110847473, Val Acc: 0.8364\n",
      "Epoch 160: Train Loss: 0.07516153156757355, Val Loss: 0.5723808407783508, Val Acc: 0.8364\n",
      "Epoch 180: Train Loss: 0.06350009888410568, Val Loss: 0.6100448369979858, Val Acc: 0.8364\n",
      "Average Validation Accuracy: 0.8866\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import APPNP\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "class APPNPNet(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_hidden, num_classes, K=10, alpha=0.1):\n",
    "        super(APPNPNet, self).__init__()\n",
    "        self.lin = torch.nn.Linear(num_node_features, num_hidden)\n",
    "        self.appnp = APPNP(K, alpha)\n",
    "        self.fc = torch.nn.Linear(num_hidden, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.lin(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc(x)\n",
    "        return self.appnp(x, edge_index)\n",
    "\n",
    "device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "k = 9\n",
    "kf = StratifiedKFold(n_splits=k)\n",
    "idx_train_np = np.array(idx_train)\n",
    "labels = data.y.numpy()[idx_train_np]\n",
    "\n",
    "best_accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(idx_train_np, labels)):\n",
    "    print(f\"Fold {fold+1}/{k}\")\n",
    "    model = APPNPNet(num_node_features=data.x.shape[1], \n",
    "                     num_hidden=64,\n",
    "                     num_classes=(data.y.max()+1).item(),\n",
    "                     K=10,\n",
    "                     alpha=0.1).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.009, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    \n",
    "    data.train_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    data.val_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    data.train_mask[idx_train_np[train_idx]] = True\n",
    "    data.val_mask[idx_train_np[val_idx]] = True\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred = model(data).argmax(dim=1)\n",
    "                valid_labels_mask = data.y[data.val_mask] != -1  # Mask to select valid labels\n",
    "                correct_predictions = pred[data.val_mask][valid_labels_mask] == data.y[data.val_mask][valid_labels_mask]\n",
    "                correct = correct_predictions.sum()\n",
    "                acc = int(correct) / int(valid_labels_mask.sum())\n",
    "                \n",
    "                if acc > best_val_acc and acc >= 0.87:\n",
    "                    best_val_acc = acc\n",
    "                    best_model_state = model.state_dict()\n",
    "                    \n",
    "                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "                print(f'Epoch {epoch}: Train Loss: {loss.item()}, Val Loss: {val_loss.item()}, Val Acc: {acc:.4f}')\n",
    "            \n",
    "    if best_model_state is not None:\n",
    "        torch.save(best_model_state, f'appnp_best_fold_{fold+1}.pt')\n",
    "    \n",
    "    if best_val_acc>0:\n",
    "        best_accuracies.append(best_val_acc)\n",
    "\n",
    "average_accuracy = sum(best_accuracies) / len(best_accuracies)\n",
    "print(f'Average Validation Accuracy: {average_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/10\n",
      "Epoch 0: Train Loss: 1.945572018623352, Val Loss: 1.9549434185028076, Val Acc: 0.5400\n",
      "Epoch 20: Train Loss: 0.31886059045791626, Val Loss: 0.4142930209636688, Val Acc: 0.9400\n",
      "Epoch 40: Train Loss: 0.26370322704315186, Val Loss: 0.6633957028388977, Val Acc: 0.8800\n",
      "Epoch 60: Train Loss: 0.18635545670986176, Val Loss: 0.4455018937587738, Val Acc: 0.9000\n",
      "Epoch 80: Train Loss: 0.1568804532289505, Val Loss: 0.43569836020469666, Val Acc: 0.8800\n",
      "Epoch 100: Train Loss: 0.12510935962200165, Val Loss: 0.5203189253807068, Val Acc: 0.8800\n",
      "Epoch 120: Train Loss: 0.14253047108650208, Val Loss: 0.4069465696811676, Val Acc: 0.9000\n",
      "Epoch 140: Train Loss: 0.11098465323448181, Val Loss: 0.6794502139091492, Val Acc: 0.8800\n",
      "Epoch 160: Train Loss: 0.14999504387378693, Val Loss: 0.47334206104278564, Val Acc: 0.9000\n",
      "Epoch 180: Train Loss: 0.16334867477416992, Val Loss: 0.6915886402130127, Val Acc: 0.8200\n",
      "Fold 2/10\n",
      "Epoch 0: Train Loss: 1.9455585479736328, Val Loss: 1.938031792640686, Val Acc: 0.5600\n",
      "Epoch 20: Train Loss: 0.29082468152046204, Val Loss: 0.7415187358856201, Val Acc: 0.8000\n",
      "Epoch 40: Train Loss: 0.22121639549732208, Val Loss: 1.0552473068237305, Val Acc: 0.7800\n",
      "Epoch 60: Train Loss: 0.2008235901594162, Val Loss: 1.1161143779754639, Val Acc: 0.7800\n",
      "Epoch 80: Train Loss: 0.15839092433452606, Val Loss: 1.14432954788208, Val Acc: 0.7400\n",
      "Epoch 100: Train Loss: 0.13748545944690704, Val Loss: 1.1034646034240723, Val Acc: 0.7400\n",
      "Epoch 120: Train Loss: 0.09206288307905197, Val Loss: 1.152907371520996, Val Acc: 0.7600\n",
      "Epoch 140: Train Loss: 0.11354509741067886, Val Loss: 1.0793510675430298, Val Acc: 0.7800\n",
      "Epoch 160: Train Loss: 0.11486681550741196, Val Loss: 1.245113730430603, Val Acc: 0.7200\n",
      "Epoch 180: Train Loss: 0.12098705023527145, Val Loss: 0.9528810381889343, Val Acc: 0.7800\n",
      "Fold 3/10\n",
      "Epoch 0: Train Loss: 1.9428300857543945, Val Loss: 1.948524832725525, Val Acc: 0.5400\n",
      "Epoch 20: Train Loss: 0.3043101727962494, Val Loss: 0.6614881157875061, Val Acc: 0.8600\n",
      "Epoch 40: Train Loss: 0.2096015363931656, Val Loss: 0.7378867268562317, Val Acc: 0.8600\n",
      "Epoch 60: Train Loss: 0.1639307290315628, Val Loss: 0.8843361139297485, Val Acc: 0.8400\n",
      "Epoch 80: Train Loss: 0.1393670290708542, Val Loss: 1.0233619213104248, Val Acc: 0.8400\n",
      "Epoch 100: Train Loss: 0.13284660875797272, Val Loss: 0.8304422497749329, Val Acc: 0.8200\n",
      "Epoch 120: Train Loss: 0.12438633292913437, Val Loss: 0.9792605042457581, Val Acc: 0.8400\n",
      "Epoch 140: Train Loss: 0.11958398669958115, Val Loss: 0.965318500995636, Val Acc: 0.8400\n",
      "Epoch 160: Train Loss: 0.11632125079631805, Val Loss: 1.0826486349105835, Val Acc: 0.7800\n",
      "Epoch 180: Train Loss: 0.1464499533176422, Val Loss: 0.789509117603302, Val Acc: 0.7800\n",
      "Fold 4/10\n",
      "Epoch 0: Train Loss: 1.9493693113327026, Val Loss: 1.9646859169006348, Val Acc: 0.4800\n",
      "Epoch 20: Train Loss: 0.3329430818557739, Val Loss: 0.5733137726783752, Val Acc: 0.8600\n",
      "Epoch 40: Train Loss: 0.21219147741794586, Val Loss: 0.6014798879623413, Val Acc: 0.8600\n",
      "Epoch 60: Train Loss: 0.1767946481704712, Val Loss: 0.5563856363296509, Val Acc: 0.8600\n",
      "Epoch 80: Train Loss: 0.14474214613437653, Val Loss: 0.8879824280738831, Val Acc: 0.8400\n",
      "Epoch 100: Train Loss: 0.13723571598529816, Val Loss: 0.5920317769050598, Val Acc: 0.8600\n",
      "Epoch 120: Train Loss: 0.1186017319560051, Val Loss: 0.4812862277030945, Val Acc: 0.8600\n",
      "Epoch 140: Train Loss: 0.1339307725429535, Val Loss: 0.9547712802886963, Val Acc: 0.8200\n",
      "Epoch 160: Train Loss: 0.10777115076780319, Val Loss: 0.55660080909729, Val Acc: 0.8600\n",
      "Epoch 180: Train Loss: 0.14778290688991547, Val Loss: 0.7756397128105164, Val Acc: 0.8400\n",
      "Fold 5/10\n",
      "Epoch 0: Train Loss: 1.939584493637085, Val Loss: 1.9445849657058716, Val Acc: 0.6000\n",
      "Epoch 20: Train Loss: 0.335724413394928, Val Loss: 0.7577002048492432, Val Acc: 0.9000\n",
      "Epoch 40: Train Loss: 0.22763487696647644, Val Loss: 0.7298502922058105, Val Acc: 0.9000\n",
      "Epoch 60: Train Loss: 0.18886835873126984, Val Loss: 0.7194572687149048, Val Acc: 0.9000\n",
      "Epoch 80: Train Loss: 0.17899668216705322, Val Loss: 0.721941351890564, Val Acc: 0.9000\n",
      "Epoch 100: Train Loss: 0.15928849577903748, Val Loss: 0.5126636624336243, Val Acc: 0.8600\n",
      "Epoch 120: Train Loss: 0.10998184978961945, Val Loss: 0.5713897943496704, Val Acc: 0.8600\n",
      "Epoch 140: Train Loss: 0.11466654390096664, Val Loss: 0.8371328115463257, Val Acc: 0.9000\n",
      "Epoch 160: Train Loss: 0.13443291187286377, Val Loss: 0.8224323391914368, Val Acc: 0.8400\n",
      "Epoch 180: Train Loss: 0.12677250802516937, Val Loss: 0.877941906452179, Val Acc: 0.8000\n",
      "Fold 6/10\n",
      "Epoch 0: Train Loss: 1.954016089439392, Val Loss: 1.9405709505081177, Val Acc: 0.4800\n",
      "Epoch 20: Train Loss: 0.2934012711048126, Val Loss: 1.1861284971237183, Val Acc: 0.8000\n",
      "Epoch 40: Train Loss: 0.19006963074207306, Val Loss: 0.8727837204933167, Val Acc: 0.8200\n",
      "Epoch 60: Train Loss: 0.20161069929599762, Val Loss: 0.7978420257568359, Val Acc: 0.8600\n",
      "Epoch 80: Train Loss: 0.19029317796230316, Val Loss: 1.126604437828064, Val Acc: 0.8200\n",
      "Epoch 100: Train Loss: 0.14708197116851807, Val Loss: 0.991513192653656, Val Acc: 0.8200\n",
      "Epoch 120: Train Loss: 0.1475292593240738, Val Loss: 0.7977014183998108, Val Acc: 0.8400\n",
      "Epoch 140: Train Loss: 0.14007651805877686, Val Loss: 0.9255388379096985, Val Acc: 0.8200\n",
      "Epoch 160: Train Loss: 0.13766761124134064, Val Loss: 0.6223240494728088, Val Acc: 0.8800\n",
      "Epoch 180: Train Loss: 0.1061912477016449, Val Loss: 1.0900346040725708, Val Acc: 0.8600\n",
      "Fold 7/10\n",
      "Epoch 0: Train Loss: 1.949389100074768, Val Loss: 1.9502605199813843, Val Acc: 0.5102\n",
      "Epoch 20: Train Loss: 0.30065667629241943, Val Loss: 0.5898669362068176, Val Acc: 0.8571\n",
      "Epoch 40: Train Loss: 0.20773780345916748, Val Loss: 0.8200861811637878, Val Acc: 0.8776\n",
      "Epoch 60: Train Loss: 0.18079951405525208, Val Loss: 0.621913731098175, Val Acc: 0.8571\n",
      "Epoch 80: Train Loss: 0.1455918550491333, Val Loss: 0.5267727375030518, Val Acc: 0.8776\n",
      "Epoch 100: Train Loss: 0.15852558612823486, Val Loss: 0.7497479319572449, Val Acc: 0.8776\n",
      "Epoch 120: Train Loss: 0.1303613930940628, Val Loss: 0.5191947221755981, Val Acc: 0.8571\n",
      "Epoch 140: Train Loss: 0.13376757502555847, Val Loss: 0.9724729061126709, Val Acc: 0.8776\n",
      "Epoch 160: Train Loss: 0.1405772566795349, Val Loss: 0.6979451775550842, Val Acc: 0.8367\n",
      "Epoch 180: Train Loss: 0.11605487018823624, Val Loss: 0.6996596455574036, Val Acc: 0.8776\n",
      "Fold 8/10\n",
      "Epoch 0: Train Loss: 1.9134044647216797, Val Loss: 1.9160624742507935, Val Acc: 0.6327\n",
      "Epoch 20: Train Loss: 0.2233181744813919, Val Loss: 1.3051388263702393, Val Acc: 0.7551\n",
      "Epoch 40: Train Loss: 0.20260754227638245, Val Loss: 0.9542486071586609, Val Acc: 0.7755\n",
      "Epoch 60: Train Loss: 0.1858637034893036, Val Loss: 1.1638685464859009, Val Acc: 0.7755\n",
      "Epoch 80: Train Loss: 0.15258906781673431, Val Loss: 1.2037990093231201, Val Acc: 0.7959\n",
      "Epoch 100: Train Loss: 0.13351832330226898, Val Loss: 1.3444488048553467, Val Acc: 0.7551\n",
      "Epoch 120: Train Loss: 0.12853792309761047, Val Loss: 1.1218054294586182, Val Acc: 0.7959\n",
      "Epoch 140: Train Loss: 0.1159726157784462, Val Loss: 1.154377818107605, Val Acc: 0.7551\n",
      "Epoch 160: Train Loss: 0.14268597960472107, Val Loss: 1.271206259727478, Val Acc: 0.7347\n",
      "Epoch 180: Train Loss: 0.12681891024112701, Val Loss: 1.1723277568817139, Val Acc: 0.7959\n",
      "Fold 9/10\n",
      "Epoch 0: Train Loss: 1.9363446235656738, Val Loss: 1.943131685256958, Val Acc: 0.5510\n",
      "Epoch 20: Train Loss: 0.2995976209640503, Val Loss: 0.8486841320991516, Val Acc: 0.8163\n",
      "Epoch 40: Train Loss: 0.2424781620502472, Val Loss: 0.42993584275245667, Val Acc: 0.8367\n",
      "Epoch 60: Train Loss: 0.16375061869621277, Val Loss: 0.764752209186554, Val Acc: 0.8163\n",
      "Epoch 80: Train Loss: 0.1690863072872162, Val Loss: 1.119455099105835, Val Acc: 0.8367\n",
      "Epoch 100: Train Loss: 0.1321934312582016, Val Loss: 0.9540595412254333, Val Acc: 0.8163\n",
      "Epoch 120: Train Loss: 0.1429169625043869, Val Loss: 0.7208254933357239, Val Acc: 0.7959\n",
      "Epoch 140: Train Loss: 0.16799688339233398, Val Loss: 0.8352020978927612, Val Acc: 0.8163\n",
      "Epoch 160: Train Loss: 0.11861532181501389, Val Loss: 0.8916499018669128, Val Acc: 0.7959\n",
      "Epoch 180: Train Loss: 0.15175087749958038, Val Loss: 0.8736816048622131, Val Acc: 0.7959\n",
      "Fold 10/10\n",
      "Epoch 0: Train Loss: 1.9516966342926025, Val Loss: 1.9605790376663208, Val Acc: 0.5102\n",
      "Epoch 20: Train Loss: 0.2968728840351105, Val Loss: 0.7124284505844116, Val Acc: 0.8571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: Train Loss: 0.283584862947464, Val Loss: 0.8661600351333618, Val Acc: 0.8776\n",
      "Epoch 60: Train Loss: 0.19523926079273224, Val Loss: 0.572965681552887, Val Acc: 0.8776\n",
      "Epoch 80: Train Loss: 0.16296552121639252, Val Loss: 0.4113086760044098, Val Acc: 0.8776\n",
      "Epoch 100: Train Loss: 0.16832558810710907, Val Loss: 0.6265553832054138, Val Acc: 0.8776\n",
      "Epoch 120: Train Loss: 0.12474682182073593, Val Loss: 0.6574022173881531, Val Acc: 0.8776\n",
      "Epoch 140: Train Loss: 0.14663568139076233, Val Loss: 0.8875391483306885, Val Acc: 0.8571\n",
      "Epoch 160: Train Loss: 0.11579129844903946, Val Loss: 0.9596827626228333, Val Acc: 0.8776\n",
      "Epoch 180: Train Loss: 0.15791785717010498, Val Loss: 0.6132438778877258, Val Acc: 0.8776\n",
      "Average Validation Accuracy: 0.8950\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_hidden, num_classes, heads=12, output_heads=1):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(num_node_features, num_hidden, heads=heads, dropout=0.2)\n",
    "        self.conv2 = GATConv(num_hidden*heads, num_classes, heads=output_heads, concat=False, dropout=0.1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # First Graph Attention Layer\n",
    "        x = F.dropout(x,training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Second Graph Attention Layer\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "\n",
    "\n",
    "device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "k = 10\n",
    "\n",
    "kf = StratifiedKFold(n_splits=k)\n",
    "idx_train_np = np.array(idx_train)\n",
    "labels = data.y.numpy()[idx_train_np]\n",
    "\n",
    "best_accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(idx_train_np, labels)):\n",
    "    print(f\"Fold {fold+1}/{k}\")\n",
    "    model = GAT(num_node_features=data.x.shape[1], \n",
    "                num_hidden=128,\n",
    "                num_classes=(data.y.max()+1).item()\n",
    "               ).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.009, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    \n",
    "    data.train_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    data.val_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    data.train_mask[idx_train_np[train_idx]] = True\n",
    "    data.val_mask[idx_train_np[val_idx]] = True\n",
    "    \n",
    "    best_val_acc = 0 \n",
    "    best_model_state = None \n",
    "    \n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "#         loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask], ignore_index=-1)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                pred = model(data).argmax(dim=1)\n",
    "                correct = (pred[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "                acc = int(correct) / int(data.val_mask.sum())\n",
    "                \n",
    "                valid_labels_mask = data.y[data.val_mask] != -1  # Mask to select valid labels not equal to -1\n",
    "                correct_predictions = pred[data.val_mask][valid_labels_mask] == data.y[data.val_mask][valid_labels_mask]\n",
    "                correct = correct_predictions.sum()\n",
    "                acc = int(correct) / int(valid_labels_mask.sum())\n",
    "                \n",
    "                if acc > best_val_acc and acc >= 0.87:\n",
    "                    best_val_acc = acc\n",
    "                    best_model_state = model.state_dict()\n",
    "                    \n",
    "                val_loss = F.nll_loss(out[data.val_mask], data.y[data.val_mask], ignore_index=-1)\n",
    "                print(f'Epoch {epoch}: Train Loss: {loss.item()}, Val Loss: {val_loss.item()}, Val Acc: {acc:.4f}')\n",
    "            \n",
    "    if best_model_state is not None:\n",
    "        torch.save(best_model_state, f'gat_best_fold_{fold+1}.pt')\n",
    "    if best_val_acc>0:\n",
    "        best_accuracies.append(best_val_acc)\n",
    "\n",
    "average_accuracy = sum(best_accuracies) / len(best_accuracies)\n",
    "print(f'Average Validation Accuracy: {average_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSage Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/9\n",
      "Epoch 0: Train Loss: 1.96201491355896, Val Loss: 1.9570649862289429, Val Acc: 0.3750\n",
      "Epoch 20: Train Loss: 0.07552532851696014, Val Loss: 0.35585102438926697, Val Acc: 0.8750\n",
      "Epoch 40: Train Loss: 0.008769199252128601, Val Loss: 0.47874197363853455, Val Acc: 0.7857\n",
      "Epoch 60: Train Loss: 0.007613993715494871, Val Loss: 0.5293914675712585, Val Acc: 0.8036\n",
      "Epoch 80: Train Loss: 0.010285736061632633, Val Loss: 0.5392610430717468, Val Acc: 0.8036\n",
      "Epoch 100: Train Loss: 0.013431865721940994, Val Loss: 0.4552469253540039, Val Acc: 0.8036\n",
      "Epoch 120: Train Loss: 0.011408250778913498, Val Loss: 0.49400249123573303, Val Acc: 0.8036\n",
      "Epoch 140: Train Loss: 0.009628426283597946, Val Loss: 0.512694239616394, Val Acc: 0.8036\n",
      "Epoch 160: Train Loss: 0.008534224703907967, Val Loss: 0.483083039522171, Val Acc: 0.8036\n",
      "Epoch 180: Train Loss: 0.010781959630548954, Val Loss: 0.6315088272094727, Val Acc: 0.8036\n",
      "Fold 2/9\n",
      "Epoch 0: Train Loss: 1.9511557817459106, Val Loss: 1.9485162496566772, Val Acc: 0.3818\n",
      "Epoch 20: Train Loss: 0.06952036917209625, Val Loss: 0.8313547968864441, Val Acc: 0.7818\n",
      "Epoch 40: Train Loss: 0.008309111930429935, Val Loss: 1.0673906803131104, Val Acc: 0.7273\n",
      "Epoch 60: Train Loss: 0.009007802233099937, Val Loss: 1.1472781896591187, Val Acc: 0.7273\n",
      "Epoch 80: Train Loss: 0.008708031848073006, Val Loss: 1.082199215888977, Val Acc: 0.7455\n",
      "Epoch 100: Train Loss: 0.011024821549654007, Val Loss: 0.9858385324478149, Val Acc: 0.7455\n",
      "Epoch 120: Train Loss: 0.009762536734342575, Val Loss: 0.8817659020423889, Val Acc: 0.7455\n",
      "Epoch 140: Train Loss: 0.010994065552949905, Val Loss: 0.8855511546134949, Val Acc: 0.7273\n",
      "Epoch 160: Train Loss: 0.010112076997756958, Val Loss: 0.9705548286437988, Val Acc: 0.7273\n",
      "Epoch 180: Train Loss: 0.00810063537210226, Val Loss: 0.8813090920448303, Val Acc: 0.7636\n",
      "Fold 3/9\n",
      "Epoch 0: Train Loss: 1.943535327911377, Val Loss: 1.9412400722503662, Val Acc: 0.4364\n",
      "Epoch 20: Train Loss: 0.059032149612903595, Val Loss: 0.6817847490310669, Val Acc: 0.7818\n",
      "Epoch 40: Train Loss: 0.0068066054955124855, Val Loss: 0.7718281149864197, Val Acc: 0.8182\n",
      "Epoch 60: Train Loss: 0.009387682192027569, Val Loss: 0.8226699233055115, Val Acc: 0.8364\n",
      "Epoch 80: Train Loss: 0.010948584415018559, Val Loss: 0.6311743259429932, Val Acc: 0.8182\n",
      "Epoch 100: Train Loss: 0.012435383163392544, Val Loss: 0.5806146264076233, Val Acc: 0.8182\n",
      "Epoch 120: Train Loss: 0.013671323657035828, Val Loss: 0.6492231488227844, Val Acc: 0.8182\n",
      "Epoch 140: Train Loss: 0.00850667804479599, Val Loss: 0.4922940731048584, Val Acc: 0.8182\n",
      "Epoch 160: Train Loss: 0.010430478490889072, Val Loss: 0.5014814734458923, Val Acc: 0.8364\n",
      "Epoch 180: Train Loss: 0.008744650520384312, Val Loss: 0.6417252421379089, Val Acc: 0.8545\n",
      "Fold 4/9\n",
      "Epoch 0: Train Loss: 1.9644901752471924, Val Loss: 1.986327052116394, Val Acc: 0.5273\n",
      "Epoch 20: Train Loss: 0.0750872790813446, Val Loss: 0.5579237937927246, Val Acc: 0.8364\n",
      "Epoch 40: Train Loss: 0.010061540640890598, Val Loss: 0.7486847639083862, Val Acc: 0.8182\n",
      "Epoch 60: Train Loss: 0.008133957162499428, Val Loss: 0.6182438135147095, Val Acc: 0.8182\n",
      "Epoch 80: Train Loss: 0.01104675605893135, Val Loss: 0.7004677653312683, Val Acc: 0.8364\n",
      "Epoch 100: Train Loss: 0.010139650665223598, Val Loss: 0.6336751580238342, Val Acc: 0.8364\n",
      "Epoch 120: Train Loss: 0.011169706471264362, Val Loss: 0.6150146722793579, Val Acc: 0.8364\n",
      "Epoch 140: Train Loss: 0.009426897391676903, Val Loss: 0.6635472178459167, Val Acc: 0.8545\n",
      "Epoch 160: Train Loss: 0.009355493821203709, Val Loss: 0.6360004544258118, Val Acc: 0.8364\n",
      "Epoch 180: Train Loss: 0.010968318209052086, Val Loss: 0.5841348767280579, Val Acc: 0.8364\n",
      "Fold 5/9\n",
      "Epoch 0: Train Loss: 1.9511773586273193, Val Loss: 1.9340723752975464, Val Acc: 0.5818\n",
      "Epoch 20: Train Loss: 0.04991305246949196, Val Loss: 0.614590048789978, Val Acc: 0.8182\n",
      "Epoch 40: Train Loss: 0.007903020828962326, Val Loss: 0.8605078458786011, Val Acc: 0.8364\n",
      "Epoch 60: Train Loss: 0.005901884753257036, Val Loss: 0.6909201145172119, Val Acc: 0.8545\n",
      "Epoch 80: Train Loss: 0.011268818750977516, Val Loss: 0.7203953266143799, Val Acc: 0.8545\n",
      "Epoch 100: Train Loss: 0.010828833095729351, Val Loss: 0.6566593647003174, Val Acc: 0.8727\n",
      "Epoch 120: Train Loss: 0.013699367642402649, Val Loss: 0.6622447371482849, Val Acc: 0.8727\n",
      "Epoch 140: Train Loss: 0.012181098572909832, Val Loss: 0.6883886456489563, Val Acc: 0.8909\n",
      "Epoch 160: Train Loss: 0.01014825887978077, Val Loss: 0.5741982460021973, Val Acc: 0.8909\n",
      "Epoch 180: Train Loss: 0.00994847435504198, Val Loss: 0.6264127492904663, Val Acc: 0.8727\n",
      "Fold 6/9\n",
      "Epoch 0: Train Loss: 1.9428679943084717, Val Loss: 1.9413037300109863, Val Acc: 0.5455\n",
      "Epoch 20: Train Loss: 0.06735213845968246, Val Loss: 0.4775465130805969, Val Acc: 0.8545\n",
      "Epoch 40: Train Loss: 0.007966053672134876, Val Loss: 0.6846744418144226, Val Acc: 0.8545\n",
      "Epoch 60: Train Loss: 0.00630146823823452, Val Loss: 0.7995818853378296, Val Acc: 0.8545\n",
      "Epoch 80: Train Loss: 0.013429687358438969, Val Loss: 0.5697681903839111, Val Acc: 0.8727\n",
      "Epoch 100: Train Loss: 0.012057234533131123, Val Loss: 0.5209065079689026, Val Acc: 0.8727\n",
      "Epoch 120: Train Loss: 0.012311086989939213, Val Loss: 0.5357296466827393, Val Acc: 0.8727\n",
      "Epoch 140: Train Loss: 0.008545862510800362, Val Loss: 0.6127638816833496, Val Acc: 0.8727\n",
      "Epoch 160: Train Loss: 0.011270062066614628, Val Loss: 0.6556733846664429, Val Acc: 0.8727\n",
      "Epoch 180: Train Loss: 0.01118259597569704, Val Loss: 0.5595269203186035, Val Acc: 0.8727\n",
      "Fold 7/9\n",
      "Epoch 0: Train Loss: 1.9443098306655884, Val Loss: 1.9316387176513672, Val Acc: 0.5455\n",
      "Epoch 20: Train Loss: 0.05292656645178795, Val Loss: 1.1735800504684448, Val Acc: 0.8000\n",
      "Epoch 40: Train Loss: 0.006716205272823572, Val Loss: 1.5033464431762695, Val Acc: 0.7818\n",
      "Epoch 60: Train Loss: 0.007935093715786934, Val Loss: 1.5127782821655273, Val Acc: 0.8182\n",
      "Epoch 80: Train Loss: 0.014097644947469234, Val Loss: 1.375145673751831, Val Acc: 0.8182\n",
      "Epoch 100: Train Loss: 0.010658152401447296, Val Loss: 1.3409518003463745, Val Acc: 0.8182\n",
      "Epoch 120: Train Loss: 0.010679350234568119, Val Loss: 1.2915925979614258, Val Acc: 0.8182\n",
      "Epoch 140: Train Loss: 0.009149699471890926, Val Loss: 1.318742275238037, Val Acc: 0.8182\n",
      "Epoch 160: Train Loss: 0.00924710277467966, Val Loss: 1.2687827348709106, Val Acc: 0.8000\n",
      "Epoch 180: Train Loss: 0.008757129311561584, Val Loss: 1.221705675125122, Val Acc: 0.8364\n",
      "Fold 8/9\n",
      "Epoch 0: Train Loss: 1.959730863571167, Val Loss: 1.9795262813568115, Val Acc: 0.4545\n",
      "Epoch 20: Train Loss: 0.06523319333791733, Val Loss: 0.4569629728794098, Val Acc: 0.8909\n",
      "Epoch 40: Train Loss: 0.011972031556069851, Val Loss: 0.6611801981925964, Val Acc: 0.8727\n",
      "Epoch 60: Train Loss: 0.010326137766242027, Val Loss: 0.5857239365577698, Val Acc: 0.8727\n",
      "Epoch 80: Train Loss: 0.014264903962612152, Val Loss: 0.6084160804748535, Val Acc: 0.8545\n",
      "Epoch 100: Train Loss: 0.0109190559014678, Val Loss: 0.5399873852729797, Val Acc: 0.8545\n",
      "Epoch 120: Train Loss: 0.011547690257430077, Val Loss: 0.5378856062889099, Val Acc: 0.8545\n",
      "Epoch 140: Train Loss: 0.01008834969252348, Val Loss: 0.6003676652908325, Val Acc: 0.8727\n",
      "Epoch 160: Train Loss: 0.010025604628026485, Val Loss: 0.5988481044769287, Val Acc: 0.8727\n",
      "Epoch 180: Train Loss: 0.009711951948702335, Val Loss: 0.5783053040504456, Val Acc: 0.8727\n",
      "Fold 9/9\n",
      "Epoch 0: Train Loss: 1.9191014766693115, Val Loss: 1.9006497859954834, Val Acc: 0.3636\n",
      "Epoch 20: Train Loss: 0.07125618308782578, Val Loss: 0.5572634339332581, Val Acc: 0.8182\n",
      "Epoch 40: Train Loss: 0.0072502815164625645, Val Loss: 0.7613893747329712, Val Acc: 0.8182\n",
      "Epoch 60: Train Loss: 0.007454735226929188, Val Loss: 0.8242276310920715, Val Acc: 0.8182\n",
      "Epoch 80: Train Loss: 0.01199923362582922, Val Loss: 0.6084756255149841, Val Acc: 0.8182\n",
      "Epoch 100: Train Loss: 0.015987904742360115, Val Loss: 0.5727213025093079, Val Acc: 0.8182\n",
      "Epoch 120: Train Loss: 0.01089213415980339, Val Loss: 0.6261113882064819, Val Acc: 0.8182\n",
      "Epoch 140: Train Loss: 0.009546049870550632, Val Loss: 0.639028787612915, Val Acc: 0.8000\n",
      "Epoch 160: Train Loss: 0.009493176825344563, Val Loss: 0.6162460446357727, Val Acc: 0.8182\n",
      "Epoch 180: Train Loss: 0.009493802674114704, Val Loss: 0.5905660390853882, Val Acc: 0.8000\n",
      "Average Validation Accuracy: 0.8731\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_hidden, num_classes):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(num_node_features, num_hidden)\n",
    "        self.conv2 = SAGEConv(num_hidden, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # First GraphSAGE Layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        # Second GraphSAGE Layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    \n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "k = 9\n",
    "kf = StratifiedKFold(n_splits=k)\n",
    "idx_train_np = np.array(idx_train)\n",
    "labels = data.y.numpy()[idx_train_np]\n",
    "\n",
    "best_accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(idx_train_np, labels)):\n",
    "    print(f\"Fold {fold+1}/{k}\")\n",
    "    model = GraphSAGE(num_node_features=data.x.shape[1], \n",
    "                num_hidden=128,\n",
    "                num_classes=(data.y.max()+1).item()\n",
    "               ).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.009, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    \n",
    "    data.train_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    data.val_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    data.train_mask[idx_train_np[train_idx]] = True\n",
    "    data.val_mask[idx_train_np[val_idx]] = True\n",
    "    \n",
    "    best_val_acc = 0 \n",
    "    best_model_state = None \n",
    "    \n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "#         loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask], ignore_index=-1)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                pred = model(data).argmax(dim=1)\n",
    "                correct = (pred[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "                acc = int(correct) / int(data.val_mask.sum())\n",
    "                \n",
    "                valid_labels_mask = data.y[data.val_mask] != -1  # Mask to select valid labels not equal to -1\n",
    "                correct_predictions = pred[data.val_mask][valid_labels_mask] == data.y[data.val_mask][valid_labels_mask]\n",
    "                correct = correct_predictions.sum()\n",
    "                acc = int(correct) / int(valid_labels_mask.sum())\n",
    "                \n",
    "                if acc > best_val_acc and acc >= 0.85:\n",
    "                    best_val_acc = acc\n",
    "                    best_model_state = model.state_dict()\n",
    "                    \n",
    "                val_loss = F.nll_loss(out[data.val_mask], data.y[data.val_mask], ignore_index=-1)\n",
    "                print(f'Epoch {epoch}: Train Loss: {loss.item()}, Val Loss: {val_loss.item()}, Val Acc: {acc:.4f}')\n",
    "            \n",
    "    if best_model_state is not None:\n",
    "        torch.save(best_model_state, f'gsage_best_fold_{fold+1}.pt')\n",
    "    if best_val_acc>0:\n",
    "        best_accuracies.append(best_val_acc)\n",
    "\n",
    "average_accuracy = sum(best_accuracies) / len(best_accuracies)\n",
    "print(f'Average Validation Accuracy: {average_accuracy:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking GCN, GAT & GSage Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from scipy.stats import mode\n",
    "\n",
    "device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = (data.y.max() + 1).item() \n",
    "\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "model_types = ['gcn', 'gat', 'gsage','appnp'] \n",
    "num_folds = 10\n",
    "\n",
    "for model_type in model_types:\n",
    "    for fold in range(1, num_folds + 1):\n",
    "        if model_type == 'gcn':\n",
    "            model = GCN(num_node_features=data.x.shape[1], \n",
    "                num_hidden=64,\n",
    "                num_classes=(data.y.max()+1).item()\n",
    "               ).to(device)\n",
    "        elif model_type == 'gat':\n",
    "            model = GAT(num_node_features=data.x.shape[1], \n",
    "                num_hidden=128,\n",
    "                num_classes=(data.y.max()+1).item()\n",
    "               ).to(device)\n",
    "        \n",
    "        elif model_type == 'appnp':\n",
    "            model = APPNPNet(num_node_features=data.x.shape[1], \n",
    "                     num_hidden=64,\n",
    "                     num_classes=(data.y.max()+1).item(),\n",
    "                     K=10,\n",
    "                     alpha=0.1).to(device)\n",
    "        elif model_type == 'gsage':\n",
    "            model = GraphSAGE(num_node_features=data.x.shape[1], \n",
    "                num_hidden=128,\n",
    "                num_classes=(data.y.max()+1).item()\n",
    "               ).to(device)\n",
    "            \n",
    "        try:\n",
    "            model_path = f'./{model_type}_best_fold_{fold}.pt' \n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = model(data.to(device))\n",
    "                preds = out.argmax(dim=1)\n",
    "                all_predictions.append(preds.cpu().numpy())\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "majority_votes, _ = mode(all_predictions, axis=0)\n",
    "majority_votes = torch.tensor(majority_votes.squeeze(), dtype=torch.long)\n",
    "# correct = (majority_votes[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "# acc = int(correct) / int(data.val_mask.sum())\n",
    "# print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# pred = model(data).argmax(dim=1)\n",
    "# correct = (pred[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "# acc = int(correct) / int(data.val_mask.sum())\n",
    "# print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = majority_votes[idx_test]\n",
    "np.savetxt('submission.txt', preds, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
