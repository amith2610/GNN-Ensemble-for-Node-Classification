{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2480"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "adj = sp.load_npz('./data_2024/adj.npz')\n",
    "feat  = np.load('./data_2024/features.npy')\n",
    "labels = np.load('./data_2024/labels.npy')\n",
    "splits = json.load(open('./data_2024/splits.json'))\n",
    "idx_train, idx_test = splits['idx_train'], splits['idx_test']\n",
    "\n",
    "\n",
    "# Dimensionality Reduction\n",
    "n_components = 128\n",
    "pca = PCA(n_components=n_components)\n",
    "reduced_feat = pca.fit_transform(feat)\n",
    "\n",
    "\n",
    "# Converting the reduced features and other arrays to torch tensors\n",
    "reduced_feat = torch.tensor(reduced_feat, dtype=torch.float)\n",
    "full_labels = -1 * np.ones(shape=(reduced_feat.shape[0],), dtype=np.int64)\n",
    "full_labels[idx_train] = labels\n",
    "labels = torch.tensor(full_labels, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "edge_index, _ = from_scipy_sparse_matrix(adj)\n",
    "\n",
    "# Converting numpy arrays to torch tensors\n",
    "# feat = torch.tensor(feat, dtype=torch.float)\n",
    "# full_labels = -1 * np.ones(shape=(feat.shape[0],), dtype=np.int64)\n",
    "# full_labels[idx_train] = labels\n",
    "# labels = torch.tensor(full_labels, dtype=torch.long)\n",
    "\n",
    "data = Data(x=reduced_feat, edge_index=edge_index, y=labels)\n",
    "# data = Data(x=feat, edge_index=edge_index, y=labels)\n",
    "\n",
    "train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "train_mask[idx_train] = True\n",
    "test_mask[idx_test] = True\n",
    "data.train_mask = train_mask\n",
    "data.test_mask = test_mask\n",
    "\n",
    "\n",
    "# num_train = int(len(idx_train) * 0.85)\n",
    "\n",
    "# train_indices = idx_train[:num_train]\n",
    "# val_indices = idx_train[num_train:]\n",
    "\n",
    "# train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "# val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "\n",
    "# train_mask[train_indices] = True\n",
    "# val_mask[val_indices] = True\n",
    "\n",
    "# data.train_mask = train_mask\n",
    "# data.val_mask = val_mask\n",
    "\n",
    "idx_train =idx_train + idx_test\n",
    "\n",
    "train_mask[idx_test] = True\n",
    "data.train_mask = train_mask\n",
    "data.train_mask.tolist().count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2480, 128], edge_index=[2, 10100], y=[2480], train_mask=[2480], test_mask=[2480])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-1, 0, 1, 2, 3, 4, 5, 6}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data.y.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_hidden, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, num_hidden)\n",
    "        self.hid1 = GCNConv(num_hidden, 16)\n",
    "        self.hid2 = GCNConv(16, num_hidden)\n",
    "        self.conv2 = GCNConv(num_hidden, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.hid1(x, edge_index)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.hid2(x, edge_index)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/8\n",
      "Epoch 0: Train Loss: 1.9226075410842896, Val Loss: 1.9369758367538452, Val Acc: 0.2903\n",
      "Epoch 10: Train Loss: 0.47048839926719666, Val Loss: 0.7952063083648682, Val Acc: 0.7903\n",
      "Epoch 20: Train Loss: 0.16516074538230896, Val Loss: 1.3619364500045776, Val Acc: 0.8548\n",
      "Epoch 30: Train Loss: 0.10967512428760529, Val Loss: 1.3378807306289673, Val Acc: 0.8226\n",
      "Epoch 40: Train Loss: 0.09042485803365707, Val Loss: 1.8531129360198975, Val Acc: 0.8065\n",
      "Epoch 50: Train Loss: 0.08192288875579834, Val Loss: 2.1105265617370605, Val Acc: 0.7742\n",
      "Epoch 60: Train Loss: 0.05542803928256035, Val Loss: 1.3494179248809814, Val Acc: 0.7903\n",
      "Epoch 70: Train Loss: 0.062417931854724884, Val Loss: 1.0445458889007568, Val Acc: 0.8065\n",
      "Epoch 80: Train Loss: 0.06241367384791374, Val Loss: 0.7780341506004333, Val Acc: 0.8226\n",
      "Epoch 90: Train Loss: 0.043040644377470016, Val Loss: 0.8189927339553833, Val Acc: 0.8387\n",
      "Epoch 100: Train Loss: 0.06919346749782562, Val Loss: 1.1516473293304443, Val Acc: 0.8065\n",
      "Epoch 110: Train Loss: 0.04432890936732292, Val Loss: 1.45476233959198, Val Acc: 0.8065\n",
      "Epoch 120: Train Loss: 0.03737865015864372, Val Loss: 1.0927300453186035, Val Acc: 0.7903\n",
      "Epoch 130: Train Loss: 0.03828039392828941, Val Loss: 1.1084212064743042, Val Acc: 0.7903\n",
      "Epoch 140: Train Loss: 0.05341823771595955, Val Loss: 0.9270000457763672, Val Acc: 0.8226\n",
      "Epoch 150: Train Loss: 0.048881031572818756, Val Loss: 1.501699686050415, Val Acc: 0.7742\n",
      "Epoch 160: Train Loss: 0.03319724649190903, Val Loss: 1.406364917755127, Val Acc: 0.8065\n",
      "Epoch 170: Train Loss: 0.048476506024599075, Val Loss: 1.0804340839385986, Val Acc: 0.8387\n",
      "Epoch 180: Train Loss: 0.027570152655243874, Val Loss: 1.138604998588562, Val Acc: 0.8065\n",
      "Epoch 190: Train Loss: 0.04643174260854721, Val Loss: 1.2937294244766235, Val Acc: 0.8387\n",
      "Fold 2/8\n",
      "Epoch 0: Train Loss: 1.9300469160079956, Val Loss: 1.9230608940124512, Val Acc: 0.3387\n",
      "Epoch 10: Train Loss: 0.45307886600494385, Val Loss: 1.255371332168579, Val Acc: 0.7581\n",
      "Epoch 20: Train Loss: 0.15199929475784302, Val Loss: 2.087797164916992, Val Acc: 0.7581\n",
      "Epoch 30: Train Loss: 0.0768715888261795, Val Loss: 2.3933911323547363, Val Acc: 0.8065\n",
      "Epoch 40: Train Loss: 0.08017342537641525, Val Loss: 2.0460925102233887, Val Acc: 0.7581\n",
      "Epoch 50: Train Loss: 0.062485769391059875, Val Loss: 2.2609903812408447, Val Acc: 0.7581\n",
      "Epoch 60: Train Loss: 0.048994194716215134, Val Loss: 1.8269551992416382, Val Acc: 0.7903\n",
      "Epoch 70: Train Loss: 0.03787361457943916, Val Loss: 2.072974443435669, Val Acc: 0.7742\n",
      "Epoch 80: Train Loss: 0.046128205955028534, Val Loss: 2.052302598953247, Val Acc: 0.7903\n",
      "Epoch 90: Train Loss: 0.053927045315504074, Val Loss: 1.8329960107803345, Val Acc: 0.7903\n",
      "Epoch 100: Train Loss: 0.04722372442483902, Val Loss: 2.3073859214782715, Val Acc: 0.7581\n",
      "Epoch 110: Train Loss: 0.031123865395784378, Val Loss: 2.967332124710083, Val Acc: 0.7419\n",
      "Epoch 120: Train Loss: 0.022620515897870064, Val Loss: 2.5260984897613525, Val Acc: 0.7742\n",
      "Epoch 130: Train Loss: 0.060627780854701996, Val Loss: 2.1009485721588135, Val Acc: 0.7742\n",
      "Epoch 140: Train Loss: 0.050852686166763306, Val Loss: 2.4680335521698, Val Acc: 0.7742\n",
      "Epoch 150: Train Loss: 0.02864092029631138, Val Loss: 2.6041007041931152, Val Acc: 0.7581\n",
      "Epoch 160: Train Loss: 0.05577090010046959, Val Loss: 2.605250120162964, Val Acc: 0.7581\n",
      "Epoch 170: Train Loss: 0.03825994208455086, Val Loss: 2.3422913551330566, Val Acc: 0.7581\n",
      "Epoch 180: Train Loss: 0.021189313381910324, Val Loss: 2.0902791023254395, Val Acc: 0.7903\n",
      "Epoch 190: Train Loss: 0.04168782755732536, Val Loss: 1.962007999420166, Val Acc: 0.7742\n",
      "Fold 3/8\n",
      "Epoch 0: Train Loss: 1.935234546661377, Val Loss: 1.9376440048217773, Val Acc: 0.2903\n",
      "Epoch 10: Train Loss: 0.411085844039917, Val Loss: 1.4007978439331055, Val Acc: 0.8548\n",
      "Epoch 20: Train Loss: 0.30639687180519104, Val Loss: 1.8051716089248657, Val Acc: 0.8226\n",
      "Epoch 30: Train Loss: 0.10583364218473434, Val Loss: 1.1252585649490356, Val Acc: 0.8387\n",
      "Epoch 40: Train Loss: 0.082685187458992, Val Loss: 1.24282968044281, Val Acc: 0.8710\n",
      "Epoch 50: Train Loss: 0.042329590767621994, Val Loss: 0.8838909268379211, Val Acc: 0.8548\n",
      "Epoch 60: Train Loss: 0.05541606247425079, Val Loss: 0.8982570767402649, Val Acc: 0.8548\n",
      "Epoch 70: Train Loss: 0.04162658378481865, Val Loss: 0.8858780860900879, Val Acc: 0.8226\n",
      "Epoch 80: Train Loss: 0.044253479689359665, Val Loss: 0.999606728553772, Val Acc: 0.8548\n",
      "Epoch 90: Train Loss: 0.07134407013654709, Val Loss: 0.9447574019432068, Val Acc: 0.8548\n",
      "Epoch 100: Train Loss: 0.042471762746572495, Val Loss: 1.083635926246643, Val Acc: 0.8226\n",
      "Epoch 110: Train Loss: 0.040845729410648346, Val Loss: 2.1894781589508057, Val Acc: 0.8226\n",
      "Epoch 120: Train Loss: 0.05340570583939552, Val Loss: 1.8510472774505615, Val Acc: 0.8226\n",
      "Epoch 130: Train Loss: 0.03083350881934166, Val Loss: 1.0599766969680786, Val Acc: 0.8226\n",
      "Epoch 140: Train Loss: 0.03227083012461662, Val Loss: 1.1396087408065796, Val Acc: 0.7903\n",
      "Epoch 150: Train Loss: 0.04361044615507126, Val Loss: 1.4046841859817505, Val Acc: 0.8065\n",
      "Epoch 160: Train Loss: 0.06015070900321007, Val Loss: 1.6700421571731567, Val Acc: 0.7742\n",
      "Epoch 170: Train Loss: 0.0515080988407135, Val Loss: 1.2842826843261719, Val Acc: 0.8226\n",
      "Epoch 180: Train Loss: 0.049700379371643066, Val Loss: 1.3000459671020508, Val Acc: 0.8065\n",
      "Epoch 190: Train Loss: 0.055592961609363556, Val Loss: 1.3121583461761475, Val Acc: 0.8387\n",
      "Fold 4/8\n",
      "Epoch 0: Train Loss: 1.9544594287872314, Val Loss: 1.9561618566513062, Val Acc: 0.4677\n",
      "Epoch 10: Train Loss: 0.6261417865753174, Val Loss: 1.890028953552246, Val Acc: 0.8710\n",
      "Epoch 20: Train Loss: 0.24961669743061066, Val Loss: 1.6873114109039307, Val Acc: 0.8548\n",
      "Epoch 30: Train Loss: 0.09578602015972137, Val Loss: 1.3339189291000366, Val Acc: 0.8387\n",
      "Epoch 40: Train Loss: 0.0749456062912941, Val Loss: 1.3865504264831543, Val Acc: 0.8226\n",
      "Epoch 50: Train Loss: 0.043723560869693756, Val Loss: 1.1201425790786743, Val Acc: 0.8387\n",
      "Epoch 60: Train Loss: 0.040197793394327164, Val Loss: 1.1357049942016602, Val Acc: 0.8387\n",
      "Epoch 70: Train Loss: 0.05552208051085472, Val Loss: 1.2293322086334229, Val Acc: 0.8387\n",
      "Epoch 80: Train Loss: 0.03922569379210472, Val Loss: 1.39125394821167, Val Acc: 0.8387\n",
      "Epoch 90: Train Loss: 0.04606087878346443, Val Loss: 1.2899279594421387, Val Acc: 0.8226\n",
      "Epoch 100: Train Loss: 0.05021272227168083, Val Loss: 1.3334630727767944, Val Acc: 0.8226\n",
      "Epoch 110: Train Loss: 0.05642594024538994, Val Loss: 1.1502618789672852, Val Acc: 0.8548\n",
      "Epoch 120: Train Loss: 0.05457792431116104, Val Loss: 1.686096429824829, Val Acc: 0.7903\n",
      "Epoch 130: Train Loss: 0.04660306125879288, Val Loss: 1.2997219562530518, Val Acc: 0.8065\n",
      "Epoch 140: Train Loss: 0.04550377279520035, Val Loss: 1.1624906063079834, Val Acc: 0.8226\n",
      "Epoch 150: Train Loss: 0.035646554082632065, Val Loss: 1.023114562034607, Val Acc: 0.8065\n",
      "Epoch 160: Train Loss: 0.04440025985240936, Val Loss: 1.269720196723938, Val Acc: 0.8065\n",
      "Epoch 170: Train Loss: 0.02957640402019024, Val Loss: 1.0489445924758911, Val Acc: 0.8065\n",
      "Epoch 180: Train Loss: 0.03418584167957306, Val Loss: 1.2749731540679932, Val Acc: 0.8226\n",
      "Epoch 190: Train Loss: 0.039321381598711014, Val Loss: 1.3482648134231567, Val Acc: 0.8226\n",
      "Fold 5/8\n",
      "Epoch 0: Train Loss: 1.907592535018921, Val Loss: 1.8950493335723877, Val Acc: 0.2903\n",
      "Epoch 10: Train Loss: 0.5927786231040955, Val Loss: 1.5898362398147583, Val Acc: 0.8871\n",
      "Epoch 20: Train Loss: 0.26807713508605957, Val Loss: 2.4635872840881348, Val Acc: 0.8548\n",
      "Epoch 30: Train Loss: 0.159376859664917, Val Loss: 2.0598642826080322, Val Acc: 0.8065\n",
      "Epoch 40: Train Loss: 0.08497437089681625, Val Loss: 2.0812571048736572, Val Acc: 0.8065\n",
      "Epoch 50: Train Loss: 0.045487020164728165, Val Loss: 1.7551982402801514, Val Acc: 0.7903\n",
      "Epoch 60: Train Loss: 0.05238273739814758, Val Loss: 2.036393642425537, Val Acc: 0.8065\n",
      "Epoch 70: Train Loss: 0.04740101471543312, Val Loss: 1.5784668922424316, Val Acc: 0.8226\n",
      "Epoch 80: Train Loss: 0.04646274074912071, Val Loss: 1.6245096921920776, Val Acc: 0.7903\n",
      "Epoch 90: Train Loss: 0.0469198040664196, Val Loss: 1.241416573524475, Val Acc: 0.8387\n",
      "Epoch 100: Train Loss: 0.05009744316339493, Val Loss: 1.482561469078064, Val Acc: 0.8548\n",
      "Epoch 110: Train Loss: 0.045234475284814835, Val Loss: 1.5467761754989624, Val Acc: 0.8226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120: Train Loss: 0.046398743987083435, Val Loss: 1.8229210376739502, Val Acc: 0.7581\n",
      "Epoch 130: Train Loss: 0.03820464015007019, Val Loss: 2.0964550971984863, Val Acc: 0.8387\n",
      "Epoch 140: Train Loss: 0.03601522371172905, Val Loss: 2.0747339725494385, Val Acc: 0.7742\n",
      "Epoch 150: Train Loss: 0.0498766228556633, Val Loss: 2.2050678730010986, Val Acc: 0.7903\n",
      "Epoch 160: Train Loss: 0.03795453533530235, Val Loss: 1.557131052017212, Val Acc: 0.8387\n",
      "Epoch 170: Train Loss: 0.03984594717621803, Val Loss: 1.3658294677734375, Val Acc: 0.7903\n",
      "Epoch 180: Train Loss: 0.02918500453233719, Val Loss: 1.7253035306930542, Val Acc: 0.7742\n",
      "Epoch 190: Train Loss: 0.03680149093270302, Val Loss: 1.2221190929412842, Val Acc: 0.8226\n",
      "Fold 6/8\n",
      "Epoch 0: Train Loss: 1.9453234672546387, Val Loss: 1.9592475891113281, Val Acc: 0.4194\n",
      "Epoch 10: Train Loss: 0.4505396783351898, Val Loss: 1.7319839000701904, Val Acc: 0.8387\n",
      "Epoch 20: Train Loss: 0.2117559015750885, Val Loss: 2.358705520629883, Val Acc: 0.8226\n",
      "Epoch 30: Train Loss: 0.11586399376392365, Val Loss: 2.534428834915161, Val Acc: 0.8226\n",
      "Epoch 40: Train Loss: 0.07072118669748306, Val Loss: 1.7717397212982178, Val Acc: 0.8387\n",
      "Epoch 50: Train Loss: 0.05420587211847305, Val Loss: 2.1780202388763428, Val Acc: 0.8226\n",
      "Epoch 60: Train Loss: 0.0449889712035656, Val Loss: 1.9905976057052612, Val Acc: 0.8387\n",
      "Epoch 70: Train Loss: 0.0922047421336174, Val Loss: 1.9983608722686768, Val Acc: 0.8387\n",
      "Epoch 80: Train Loss: 0.07960877567529678, Val Loss: 1.6503899097442627, Val Acc: 0.8387\n",
      "Epoch 90: Train Loss: 0.06978736072778702, Val Loss: 2.6423733234405518, Val Acc: 0.8387\n",
      "Epoch 100: Train Loss: 0.04606712982058525, Val Loss: 2.1017017364501953, Val Acc: 0.8226\n",
      "Epoch 110: Train Loss: 0.042260006070137024, Val Loss: 2.3164358139038086, Val Acc: 0.8387\n",
      "Epoch 120: Train Loss: 0.03656895458698273, Val Loss: 2.4101064205169678, Val Acc: 0.8548\n",
      "Epoch 130: Train Loss: 0.04273609444499016, Val Loss: 1.9763158559799194, Val Acc: 0.8387\n",
      "Epoch 140: Train Loss: 0.03753579407930374, Val Loss: 1.7529151439666748, Val Acc: 0.8548\n",
      "Epoch 150: Train Loss: 0.028180917724967003, Val Loss: 2.0632376670837402, Val Acc: 0.8387\n",
      "Epoch 160: Train Loss: 0.026857707649469376, Val Loss: 1.6447362899780273, Val Acc: 0.8387\n",
      "Epoch 170: Train Loss: 0.02470269240438938, Val Loss: 1.6900641918182373, Val Acc: 0.8226\n",
      "Epoch 180: Train Loss: 0.055919989943504333, Val Loss: 1.7825939655303955, Val Acc: 0.8548\n",
      "Epoch 190: Train Loss: 0.03723523020744324, Val Loss: 1.578559398651123, Val Acc: 0.8548\n",
      "Fold 7/8\n",
      "Epoch 0: Train Loss: 1.9367626905441284, Val Loss: 1.9549182653427124, Val Acc: 0.3065\n",
      "Epoch 10: Train Loss: 0.2887110114097595, Val Loss: 1.3285995721817017, Val Acc: 0.7742\n",
      "Epoch 20: Train Loss: 0.158153235912323, Val Loss: 3.0185554027557373, Val Acc: 0.7581\n",
      "Epoch 30: Train Loss: 0.09660739451646805, Val Loss: 2.5047805309295654, Val Acc: 0.7419\n",
      "Epoch 40: Train Loss: 0.06830818206071854, Val Loss: 2.3094890117645264, Val Acc: 0.7581\n",
      "Epoch 50: Train Loss: 0.041525959968566895, Val Loss: 2.1087231636047363, Val Acc: 0.7903\n",
      "Epoch 60: Train Loss: 0.07039586454629898, Val Loss: 1.7792130708694458, Val Acc: 0.7903\n",
      "Epoch 70: Train Loss: 0.07621636241674423, Val Loss: 2.0675764083862305, Val Acc: 0.7742\n",
      "Epoch 80: Train Loss: 0.07064630836248398, Val Loss: 2.5432515144348145, Val Acc: 0.7742\n",
      "Epoch 90: Train Loss: 0.03319961950182915, Val Loss: 2.5235726833343506, Val Acc: 0.7742\n",
      "Epoch 100: Train Loss: 0.03468861058354378, Val Loss: 2.1376655101776123, Val Acc: 0.7903\n",
      "Epoch 110: Train Loss: 0.03136821463704109, Val Loss: 1.836276650428772, Val Acc: 0.7581\n",
      "Epoch 120: Train Loss: 0.04026731103658676, Val Loss: 1.6106895208358765, Val Acc: 0.7903\n",
      "Epoch 130: Train Loss: 0.03123442642390728, Val Loss: 2.0649290084838867, Val Acc: 0.7581\n",
      "Epoch 140: Train Loss: 0.04150364175438881, Val Loss: 2.1417622566223145, Val Acc: 0.8065\n",
      "Epoch 150: Train Loss: 0.02292952500283718, Val Loss: 2.05910587310791, Val Acc: 0.7419\n",
      "Epoch 160: Train Loss: 0.03145365044474602, Val Loss: 1.92552649974823, Val Acc: 0.7903\n",
      "Epoch 170: Train Loss: 0.043478067964315414, Val Loss: 2.262964963912964, Val Acc: 0.7581\n",
      "Epoch 180: Train Loss: 0.030643455684185028, Val Loss: 1.6960246562957764, Val Acc: 0.7419\n",
      "Epoch 190: Train Loss: 0.03794329613447189, Val Loss: 2.305657148361206, Val Acc: 0.7581\n",
      "Fold 8/8\n",
      "Epoch 0: Train Loss: 1.9566404819488525, Val Loss: 1.9339172840118408, Val Acc: 0.4516\n",
      "Epoch 10: Train Loss: 0.5083746910095215, Val Loss: 1.6409084796905518, Val Acc: 0.8387\n",
      "Epoch 20: Train Loss: 0.20574694871902466, Val Loss: 1.5051970481872559, Val Acc: 0.8226\n",
      "Epoch 30: Train Loss: 0.12212224304676056, Val Loss: 1.739831805229187, Val Acc: 0.8387\n",
      "Epoch 40: Train Loss: 0.08377901464700699, Val Loss: 1.014178991317749, Val Acc: 0.8871\n",
      "Epoch 50: Train Loss: 0.053095050156116486, Val Loss: 1.0601741075515747, Val Acc: 0.8548\n",
      "Epoch 60: Train Loss: 0.045187562704086304, Val Loss: 1.0745761394500732, Val Acc: 0.8065\n",
      "Epoch 70: Train Loss: 0.04498697444796562, Val Loss: 0.967716634273529, Val Acc: 0.8226\n",
      "Epoch 80: Train Loss: 0.07633320242166519, Val Loss: 1.188957929611206, Val Acc: 0.8226\n",
      "Epoch 90: Train Loss: 0.04949694499373436, Val Loss: 0.9065386652946472, Val Acc: 0.8387\n",
      "Epoch 100: Train Loss: 0.04804006218910217, Val Loss: 1.0280568599700928, Val Acc: 0.7903\n",
      "Epoch 110: Train Loss: 0.04881475493311882, Val Loss: 0.9344109296798706, Val Acc: 0.8387\n",
      "Epoch 120: Train Loss: 0.0782264843583107, Val Loss: 1.0104570388793945, Val Acc: 0.8065\n",
      "Epoch 130: Train Loss: 0.04222588241100311, Val Loss: 1.0457773208618164, Val Acc: 0.8065\n",
      "Epoch 140: Train Loss: 0.04943399131298065, Val Loss: 1.1223276853561401, Val Acc: 0.7742\n",
      "Epoch 150: Train Loss: 0.05562417209148407, Val Loss: 1.141960620880127, Val Acc: 0.8710\n",
      "Epoch 160: Train Loss: 0.0731557160615921, Val Loss: 0.9982994198799133, Val Acc: 0.9032\n",
      "Epoch 170: Train Loss: 0.05572190880775452, Val Loss: 0.8322904706001282, Val Acc: 0.8710\n",
      "Epoch 180: Train Loss: 0.03939047083258629, Val Loss: 1.3185219764709473, Val Acc: 0.8065\n",
      "Epoch 190: Train Loss: 0.05411499738693237, Val Loss: 0.911348283290863, Val Acc: 0.8548\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "k = 8\n",
    "kf = StratifiedKFold(n_splits=k)\n",
    "idx_train_np = np.array(idx_train)\n",
    "labels = data.y.numpy()[idx_train_np]\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(idx_train_np, labels)):\n",
    "    print(f\"Fold {fold+1}/{k}\")\n",
    "\n",
    "    model = GCN(num_node_features=data.x.shape[1], \n",
    "                num_hidden=64,\n",
    "                num_classes=(data.y.max()+1).item()\n",
    "               ).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.09, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    \n",
    "    \n",
    "    data.train_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    data.val_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    data.train_mask[idx_train_np[train_idx]] = True\n",
    "    data.val_mask[idx_train_np[val_idx]] = True\n",
    "    \n",
    "    best_val_acc = 0 \n",
    "    best_model_state = None \n",
    "    \n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "#         loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask], ignore_index=-1)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                pred = model(data).argmax(dim=1)\n",
    "#                 correct = (pred[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "#                 acc = int(correct) / int(data.val_mask.sum())\n",
    "                \n",
    "                valid_labels_mask = data.y[data.val_mask] != -1  # Mask to select valid labels not equal to -1\n",
    "                correct_predictions = pred[data.val_mask][valid_labels_mask] == data.y[data.val_mask][valid_labels_mask]\n",
    "                correct = correct_predictions.sum()\n",
    "                acc = int(correct) / int(valid_labels_mask.sum())\n",
    "                \n",
    "                if acc > best_val_acc and acc >= 0.84:\n",
    "                    best_val_acc = acc\n",
    "                    best_model_state = model.state_dict()\n",
    "                    \n",
    "                val_loss = F.nll_loss(out[data.val_mask], data.y[data.val_mask], ignore_index=-1)\n",
    "                print(f'Epoch {epoch}: Train Loss: {loss.item()}, Val Loss: {val_loss.item()}, Val Acc: {acc:.4f}')\n",
    "            \n",
    "    if best_model_state is not None:\n",
    "        torch.save(best_model_state, f'gcn_best_fold_{fold+1}.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPNP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/9\n",
      "Epoch 0: Train Loss: 1.9717018604278564, Val Loss: 1.9771214723587036, Val Acc: 0.2857\n",
      "Epoch 20: Train Loss: 0.882332444190979, Val Loss: 0.9461859464645386, Val Acc: 0.7679\n",
      "Epoch 40: Train Loss: 0.33162984251976013, Val Loss: 0.48905590176582336, Val Acc: 0.8929\n",
      "Epoch 60: Train Loss: 0.17242231965065002, Val Loss: 0.4200466573238373, Val Acc: 0.8750\n",
      "Epoch 80: Train Loss: 0.11951716244220734, Val Loss: 0.4165192246437073, Val Acc: 0.8571\n",
      "Epoch 100: Train Loss: 0.10337674617767334, Val Loss: 0.4168970286846161, Val Acc: 0.8571\n",
      "Epoch 120: Train Loss: 0.09915605187416077, Val Loss: 0.43586984276771545, Val Acc: 0.8393\n",
      "Epoch 140: Train Loss: 0.08230268955230713, Val Loss: 0.4799705147743225, Val Acc: 0.8393\n",
      "Epoch 160: Train Loss: 0.08680712431669235, Val Loss: 0.4816918969154358, Val Acc: 0.8214\n",
      "Epoch 180: Train Loss: 0.07797427475452423, Val Loss: 0.43975695967674255, Val Acc: 0.8393\n",
      "Fold 2/9\n",
      "Epoch 0: Train Loss: 1.955379605293274, Val Loss: 1.9581576585769653, Val Acc: 0.2727\n",
      "Epoch 20: Train Loss: 0.8296985626220703, Val Loss: 0.9933924078941345, Val Acc: 0.6909\n",
      "Epoch 40: Train Loss: 0.305827260017395, Val Loss: 0.6114626526832581, Val Acc: 0.9091\n",
      "Epoch 60: Train Loss: 0.16468551754951477, Val Loss: 0.6315193772315979, Val Acc: 0.8727\n",
      "Epoch 80: Train Loss: 0.11360205709934235, Val Loss: 0.5193603038787842, Val Acc: 0.8364\n",
      "Epoch 100: Train Loss: 0.10471408069133759, Val Loss: 0.6013386249542236, Val Acc: 0.8364\n",
      "Epoch 120: Train Loss: 0.09165830165147781, Val Loss: 0.5503938794136047, Val Acc: 0.8182\n",
      "Epoch 140: Train Loss: 0.0766599029302597, Val Loss: 0.592608630657196, Val Acc: 0.8182\n",
      "Epoch 160: Train Loss: 0.08360560983419418, Val Loss: 0.6306477189064026, Val Acc: 0.8182\n",
      "Epoch 180: Train Loss: 0.07183001190423965, Val Loss: 0.7530838251113892, Val Acc: 0.8182\n",
      "Fold 3/9\n",
      "Epoch 0: Train Loss: 1.9501440525054932, Val Loss: 1.9533497095108032, Val Acc: 0.3091\n",
      "Epoch 20: Train Loss: 0.8372364640235901, Val Loss: 0.9206410050392151, Val Acc: 0.7636\n",
      "Epoch 40: Train Loss: 0.3264773488044739, Val Loss: 0.5392884612083435, Val Acc: 0.8727\n",
      "Epoch 60: Train Loss: 0.18139499425888062, Val Loss: 0.48432841897010803, Val Acc: 0.8727\n",
      "Epoch 80: Train Loss: 0.11489313840866089, Val Loss: 0.483539879322052, Val Acc: 0.8364\n",
      "Epoch 100: Train Loss: 0.10510813444852829, Val Loss: 0.48114967346191406, Val Acc: 0.8727\n",
      "Epoch 120: Train Loss: 0.08615829050540924, Val Loss: 0.5835009217262268, Val Acc: 0.8545\n",
      "Epoch 140: Train Loss: 0.07787487655878067, Val Loss: 0.5904735326766968, Val Acc: 0.8182\n",
      "Epoch 160: Train Loss: 0.07520808279514313, Val Loss: 0.4422244727611542, Val Acc: 0.8545\n",
      "Epoch 180: Train Loss: 0.07895597815513611, Val Loss: 0.5895795822143555, Val Acc: 0.8364\n",
      "Fold 4/9\n",
      "Epoch 0: Train Loss: 1.9496783018112183, Val Loss: 1.9489483833312988, Val Acc: 0.1818\n",
      "Epoch 20: Train Loss: 0.8595771789550781, Val Loss: 0.9568408131599426, Val Acc: 0.7636\n",
      "Epoch 40: Train Loss: 0.31333452463150024, Val Loss: 0.43913620710372925, Val Acc: 0.8727\n",
      "Epoch 60: Train Loss: 0.1642894148826599, Val Loss: 0.3985564112663269, Val Acc: 0.8364\n",
      "Epoch 80: Train Loss: 0.11580679565668106, Val Loss: 0.3760363757610321, Val Acc: 0.8364\n",
      "Epoch 100: Train Loss: 0.10674431174993515, Val Loss: 0.3813118636608124, Val Acc: 0.8364\n",
      "Epoch 120: Train Loss: 0.09408777207136154, Val Loss: 0.44311821460723877, Val Acc: 0.8364\n",
      "Epoch 140: Train Loss: 0.07863757759332657, Val Loss: 0.45181822776794434, Val Acc: 0.8364\n",
      "Epoch 160: Train Loss: 0.09405791759490967, Val Loss: 0.45562833547592163, Val Acc: 0.8364\n",
      "Epoch 180: Train Loss: 0.0790242850780487, Val Loss: 0.3940497636795044, Val Acc: 0.8364\n",
      "Fold 5/9\n",
      "Epoch 0: Train Loss: 1.9519157409667969, Val Loss: 1.948548436164856, Val Acc: 0.3273\n",
      "Epoch 20: Train Loss: 0.8888384699821472, Val Loss: 0.8896611332893372, Val Acc: 0.8364\n",
      "Epoch 40: Train Loss: 0.3336576521396637, Val Loss: 0.4440935254096985, Val Acc: 0.8727\n",
      "Epoch 60: Train Loss: 0.1778540462255478, Val Loss: 0.4537884593009949, Val Acc: 0.8909\n",
      "Epoch 80: Train Loss: 0.12836414575576782, Val Loss: 0.3917388617992401, Val Acc: 0.8727\n",
      "Epoch 100: Train Loss: 0.10483094304800034, Val Loss: 0.4375784695148468, Val Acc: 0.8727\n",
      "Epoch 120: Train Loss: 0.08776213973760605, Val Loss: 0.486531138420105, Val Acc: 0.8727\n",
      "Epoch 140: Train Loss: 0.07695586234331131, Val Loss: 0.443991094827652, Val Acc: 0.8545\n",
      "Epoch 160: Train Loss: 0.07914165407419205, Val Loss: 0.47714361548423767, Val Acc: 0.8545\n",
      "Epoch 180: Train Loss: 0.07610826194286346, Val Loss: 0.44031447172164917, Val Acc: 0.8545\n",
      "Fold 6/9\n",
      "Epoch 0: Train Loss: 1.932320475578308, Val Loss: 1.9281343221664429, Val Acc: 0.3818\n",
      "Epoch 20: Train Loss: 0.8573353886604309, Val Loss: 0.9353019595146179, Val Acc: 0.7455\n",
      "Epoch 40: Train Loss: 0.305997371673584, Val Loss: 0.48735764622688293, Val Acc: 0.8727\n",
      "Epoch 60: Train Loss: 0.1689731776714325, Val Loss: 0.5031194090843201, Val Acc: 0.8727\n",
      "Epoch 80: Train Loss: 0.11767575889825821, Val Loss: 0.5848846435546875, Val Acc: 0.8545\n",
      "Epoch 100: Train Loss: 0.09294506162405014, Val Loss: 0.48932841420173645, Val Acc: 0.8545\n",
      "Epoch 120: Train Loss: 0.0854545310139656, Val Loss: 0.5259605646133423, Val Acc: 0.8545\n",
      "Epoch 140: Train Loss: 0.07286936044692993, Val Loss: 0.6153936982154846, Val Acc: 0.8545\n",
      "Epoch 160: Train Loss: 0.08389624208211899, Val Loss: 0.6648533940315247, Val Acc: 0.8545\n",
      "Epoch 180: Train Loss: 0.0705324113368988, Val Loss: 0.6100862622261047, Val Acc: 0.8545\n",
      "Fold 7/9\n",
      "Epoch 0: Train Loss: 1.9651670455932617, Val Loss: 1.9594151973724365, Val Acc: 0.1636\n",
      "Epoch 20: Train Loss: 0.8881517648696899, Val Loss: 1.0877716541290283, Val Acc: 0.7091\n",
      "Epoch 40: Train Loss: 0.2956617474555969, Val Loss: 0.7905030846595764, Val Acc: 0.8000\n",
      "Epoch 60: Train Loss: 0.15863487124443054, Val Loss: 0.8656624555587769, Val Acc: 0.8000\n",
      "Epoch 80: Train Loss: 0.11388590931892395, Val Loss: 1.0360052585601807, Val Acc: 0.8182\n",
      "Epoch 100: Train Loss: 0.09169808775186539, Val Loss: 1.0712288618087769, Val Acc: 0.8182\n",
      "Epoch 120: Train Loss: 0.08976740390062332, Val Loss: 0.9275410175323486, Val Acc: 0.8000\n",
      "Epoch 140: Train Loss: 0.0691104456782341, Val Loss: 1.0240130424499512, Val Acc: 0.8000\n",
      "Epoch 160: Train Loss: 0.07142256945371628, Val Loss: 1.021626353263855, Val Acc: 0.8000\n",
      "Epoch 180: Train Loss: 0.06636727601289749, Val Loss: 1.147600531578064, Val Acc: 0.8000\n",
      "Fold 8/9\n",
      "Epoch 0: Train Loss: 1.931010365486145, Val Loss: 1.936535358428955, Val Acc: 0.3636\n",
      "Epoch 20: Train Loss: 0.8419895768165588, Val Loss: 0.8919957280158997, Val Acc: 0.7455\n",
      "Epoch 40: Train Loss: 0.32195353507995605, Val Loss: 0.4181394875049591, Val Acc: 0.8727\n",
      "Epoch 60: Train Loss: 0.17387881875038147, Val Loss: 0.3469115197658539, Val Acc: 0.8545\n",
      "Epoch 80: Train Loss: 0.12838901579380035, Val Loss: 0.35427796840667725, Val Acc: 0.8545\n",
      "Epoch 100: Train Loss: 0.10395007580518723, Val Loss: 0.46481063961982727, Val Acc: 0.8545\n",
      "Epoch 120: Train Loss: 0.08848130702972412, Val Loss: 0.4775886833667755, Val Acc: 0.8545\n",
      "Epoch 140: Train Loss: 0.09015596657991409, Val Loss: 0.5048468708992004, Val Acc: 0.8545\n",
      "Epoch 160: Train Loss: 0.08169417083263397, Val Loss: 0.5223650336265564, Val Acc: 0.8545\n",
      "Epoch 180: Train Loss: 0.0714678093791008, Val Loss: 0.5903120636940002, Val Acc: 0.8545\n",
      "Fold 9/9\n",
      "Epoch 0: Train Loss: 1.9474862813949585, Val Loss: 1.9433704614639282, Val Acc: 0.2727\n",
      "Epoch 20: Train Loss: 0.8253583312034607, Val Loss: 0.9238970875740051, Val Acc: 0.7273\n",
      "Epoch 40: Train Loss: 0.2951972484588623, Val Loss: 0.49545353651046753, Val Acc: 0.8545\n",
      "Epoch 60: Train Loss: 0.1589735746383667, Val Loss: 0.4412859082221985, Val Acc: 0.8545\n",
      "Epoch 80: Train Loss: 0.11732392758131027, Val Loss: 0.468158483505249, Val Acc: 0.8364\n",
      "Epoch 100: Train Loss: 0.10571148991584778, Val Loss: 0.48647505044937134, Val Acc: 0.8364\n",
      "Epoch 120: Train Loss: 0.08430692553520203, Val Loss: 0.6001875400543213, Val Acc: 0.8364\n",
      "Epoch 140: Train Loss: 0.08039739727973938, Val Loss: 0.44742581248283386, Val Acc: 0.8364\n",
      "Epoch 160: Train Loss: 0.07758013159036636, Val Loss: 0.5517902970314026, Val Acc: 0.8364\n",
      "Epoch 180: Train Loss: 0.06373756378889084, Val Loss: 0.5975775718688965, Val Acc: 0.8364\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import APPNP\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "class APPNPNet(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_hidden, num_classes, K=10, alpha=0.1):\n",
    "        super(APPNPNet, self).__init__()\n",
    "        self.lin = torch.nn.Linear(num_node_features, num_hidden)\n",
    "        self.appnp = APPNP(K, alpha)\n",
    "        self.fc = torch.nn.Linear(num_hidden, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.lin(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc(x)\n",
    "        return self.appnp(x, edge_index)\n",
    "\n",
    "device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "k = 9\n",
    "kf = StratifiedKFold(n_splits=k)\n",
    "idx_train_np = np.array(idx_train)\n",
    "labels = data.y.numpy()[idx_train_np]\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(idx_train_np, labels)):\n",
    "    print(f\"Fold {fold+1}/{k}\")\n",
    "    model = APPNPNet(num_node_features=data.x.shape[1], \n",
    "                     num_hidden=64,\n",
    "                     num_classes=(data.y.max()+1).item(),\n",
    "                     K=10,\n",
    "                     alpha=0.1).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.009, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    \n",
    "    data.train_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    data.val_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    data.train_mask[idx_train_np[train_idx]] = True\n",
    "    data.val_mask[idx_train_np[val_idx]] = True\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred = model(data).argmax(dim=1)\n",
    "                valid_labels_mask = data.y[data.val_mask] != -1  # Mask to select valid labels\n",
    "                correct_predictions = pred[data.val_mask][valid_labels_mask] == data.y[data.val_mask][valid_labels_mask]\n",
    "                correct = correct_predictions.sum()\n",
    "                acc = int(correct) / int(valid_labels_mask.sum())\n",
    "                \n",
    "                if acc > best_val_acc and acc >= 0.87:\n",
    "                    best_val_acc = acc\n",
    "                    best_model_state = model.state_dict()\n",
    "                    \n",
    "                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "                print(f'Epoch {epoch}: Train Loss: {loss.item()}, Val Loss: {val_loss.item()}, Val Acc: {acc:.4f}')\n",
    "            \n",
    "    if best_model_state is not None:\n",
    "        torch.save(best_model_state, f'appnp_best_fold_{fold+1}.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/10\n",
      "Epoch 0: Train Loss: 1.9339195489883423, Val Loss: 1.8989968299865723, Val Acc: 0.5600\n",
      "Epoch 20: Train Loss: 0.2688402831554413, Val Loss: 0.5558732748031616, Val Acc: 0.9200\n",
      "Epoch 40: Train Loss: 0.2402266412973404, Val Loss: 0.6509051322937012, Val Acc: 0.9200\n",
      "Epoch 60: Train Loss: 0.19448530673980713, Val Loss: 0.4748438894748688, Val Acc: 0.8400\n",
      "Epoch 80: Train Loss: 0.15065352618694305, Val Loss: 0.3313513696193695, Val Acc: 0.8800\n",
      "Epoch 100: Train Loss: 0.1586482673883438, Val Loss: 0.5552340149879456, Val Acc: 0.9000\n",
      "Epoch 120: Train Loss: 0.17382048070430756, Val Loss: 0.5982710719108582, Val Acc: 0.8800\n",
      "Epoch 140: Train Loss: 0.12806735932826996, Val Loss: 0.6223601698875427, Val Acc: 0.8800\n",
      "Epoch 160: Train Loss: 0.14321906864643097, Val Loss: 0.6883198618888855, Val Acc: 0.8200\n",
      "Epoch 180: Train Loss: 0.14756493270397186, Val Loss: 0.49281734228134155, Val Acc: 0.8800\n",
      "Fold 2/10\n",
      "Epoch 0: Train Loss: 1.9428246021270752, Val Loss: 1.9343044757843018, Val Acc: 0.5200\n",
      "Epoch 20: Train Loss: 0.32088354229927063, Val Loss: 1.202269434928894, Val Acc: 0.8000\n",
      "Epoch 40: Train Loss: 0.2338382750749588, Val Loss: 0.8716946244239807, Val Acc: 0.7800\n",
      "Epoch 60: Train Loss: 0.15568986535072327, Val Loss: 0.998577356338501, Val Acc: 0.8200\n",
      "Epoch 80: Train Loss: 0.16068971157073975, Val Loss: 0.9421843886375427, Val Acc: 0.7800\n",
      "Epoch 100: Train Loss: 0.1386529803276062, Val Loss: 1.1750982999801636, Val Acc: 0.8000\n",
      "Epoch 120: Train Loss: 0.12073469161987305, Val Loss: 1.2586758136749268, Val Acc: 0.7800\n",
      "Epoch 140: Train Loss: 0.13110174238681793, Val Loss: 0.8590787649154663, Val Acc: 0.7800\n",
      "Epoch 160: Train Loss: 0.13284525275230408, Val Loss: 1.5295019149780273, Val Acc: 0.7600\n",
      "Epoch 180: Train Loss: 0.11157713085412979, Val Loss: 1.2682583332061768, Val Acc: 0.8000\n",
      "Fold 3/10\n",
      "Epoch 0: Train Loss: 1.949602723121643, Val Loss: 1.945521354675293, Val Acc: 0.5200\n",
      "Epoch 20: Train Loss: 0.3345347046852112, Val Loss: 0.7360764145851135, Val Acc: 0.8200\n",
      "Epoch 40: Train Loss: 0.19796274602413177, Val Loss: 0.984955370426178, Val Acc: 0.8000\n",
      "Epoch 60: Train Loss: 0.17861025035381317, Val Loss: 0.8530867695808411, Val Acc: 0.8000\n",
      "Epoch 80: Train Loss: 0.17871502041816711, Val Loss: 0.7512940764427185, Val Acc: 0.8200\n",
      "Epoch 100: Train Loss: 0.1337336003780365, Val Loss: 0.9375073909759521, Val Acc: 0.8400\n",
      "Epoch 120: Train Loss: 0.13464753329753876, Val Loss: 1.1578707695007324, Val Acc: 0.8000\n",
      "Epoch 140: Train Loss: 0.12807290256023407, Val Loss: 1.0882068872451782, Val Acc: 0.8200\n",
      "Epoch 160: Train Loss: 0.11074553430080414, Val Loss: 1.0958408117294312, Val Acc: 0.8200\n",
      "Epoch 180: Train Loss: 0.11168172955513, Val Loss: 1.185036063194275, Val Acc: 0.8200\n",
      "Fold 4/10\n",
      "Epoch 0: Train Loss: 1.9481121301651, Val Loss: 1.9668188095092773, Val Acc: 0.5200\n",
      "Epoch 20: Train Loss: 0.28416526317596436, Val Loss: 0.5687940716743469, Val Acc: 0.8200\n",
      "Epoch 40: Train Loss: 0.17663416266441345, Val Loss: 0.6209663152694702, Val Acc: 0.8400\n",
      "Epoch 60: Train Loss: 0.16525869071483612, Val Loss: 0.6104050278663635, Val Acc: 0.8400\n",
      "Epoch 80: Train Loss: 0.1384914517402649, Val Loss: 0.5096637606620789, Val Acc: 0.8400\n",
      "Epoch 100: Train Loss: 0.12230892479419708, Val Loss: 0.6083390712738037, Val Acc: 0.8200\n",
      "Epoch 120: Train Loss: 0.12857428193092346, Val Loss: 0.8070890307426453, Val Acc: 0.8400\n",
      "Epoch 140: Train Loss: 0.14434155821800232, Val Loss: 0.8046092391014099, Val Acc: 0.8200\n",
      "Epoch 160: Train Loss: 0.10974288731813431, Val Loss: 0.9434833526611328, Val Acc: 0.8800\n",
      "Epoch 180: Train Loss: 0.12615160644054413, Val Loss: 1.0475430488586426, Val Acc: 0.8600\n",
      "Fold 5/10\n",
      "Epoch 0: Train Loss: 1.9410698413848877, Val Loss: 1.9634929895401, Val Acc: 0.5800\n",
      "Epoch 20: Train Loss: 0.3248542249202728, Val Loss: 0.736539900302887, Val Acc: 0.9000\n",
      "Epoch 40: Train Loss: 0.23889845609664917, Val Loss: 1.0275064706802368, Val Acc: 0.8800\n",
      "Epoch 60: Train Loss: 0.21704959869384766, Val Loss: 0.6390405297279358, Val Acc: 0.8600\n",
      "Epoch 80: Train Loss: 0.18854928016662598, Val Loss: 0.6979680061340332, Val Acc: 0.9000\n",
      "Epoch 100: Train Loss: 0.1602475345134735, Val Loss: 0.7989189624786377, Val Acc: 0.8600\n",
      "Epoch 120: Train Loss: 0.1373048722743988, Val Loss: 0.846386730670929, Val Acc: 0.8400\n",
      "Epoch 140: Train Loss: 0.1434367597103119, Val Loss: 0.7001751661300659, Val Acc: 0.8800\n",
      "Epoch 160: Train Loss: 0.15580689907073975, Val Loss: 0.8456661701202393, Val Acc: 0.8600\n",
      "Epoch 180: Train Loss: 0.12274619936943054, Val Loss: 0.7833319902420044, Val Acc: 0.8400\n",
      "Fold 6/10\n",
      "Epoch 0: Train Loss: 1.9476971626281738, Val Loss: 1.9593117237091064, Val Acc: 0.4800\n",
      "Epoch 20: Train Loss: 0.2986917197704315, Val Loss: 1.0646686553955078, Val Acc: 0.8400\n",
      "Epoch 40: Train Loss: 0.20079001784324646, Val Loss: 0.586458146572113, Val Acc: 0.8600\n",
      "Epoch 60: Train Loss: 0.15582957863807678, Val Loss: 0.7171227335929871, Val Acc: 0.8000\n",
      "Epoch 80: Train Loss: 0.13291388750076294, Val Loss: 0.872529923915863, Val Acc: 0.8600\n",
      "Epoch 100: Train Loss: 0.1556020975112915, Val Loss: 0.6044049263000488, Val Acc: 0.8200\n",
      "Epoch 120: Train Loss: 0.1586674302816391, Val Loss: 0.812717616558075, Val Acc: 0.8800\n",
      "Epoch 140: Train Loss: 0.1438521295785904, Val Loss: 0.7697175741195679, Val Acc: 0.8400\n",
      "Epoch 160: Train Loss: 0.14150495827198029, Val Loss: 1.1685569286346436, Val Acc: 0.8200\n",
      "Epoch 180: Train Loss: 0.12957265973091125, Val Loss: 1.0039286613464355, Val Acc: 0.8200\n",
      "Fold 7/10\n",
      "Epoch 0: Train Loss: 1.969252109527588, Val Loss: 1.9774281978607178, Val Acc: 0.6122\n",
      "Epoch 20: Train Loss: 0.24911224842071533, Val Loss: 0.9504504799842834, Val Acc: 0.8367\n",
      "Epoch 40: Train Loss: 0.19814345240592957, Val Loss: 0.60664302110672, Val Acc: 0.8776\n",
      "Epoch 60: Train Loss: 0.19839239120483398, Val Loss: 0.6664853096008301, Val Acc: 0.8571\n",
      "Epoch 80: Train Loss: 0.17064738273620605, Val Loss: 0.6734315752983093, Val Acc: 0.8571\n",
      "Epoch 100: Train Loss: 0.15171508491039276, Val Loss: 0.9678093791007996, Val Acc: 0.8367\n",
      "Epoch 120: Train Loss: 0.1355554163455963, Val Loss: 0.5487920045852661, Val Acc: 0.8367\n",
      "Epoch 140: Train Loss: 0.13849835097789764, Val Loss: 0.6272331476211548, Val Acc: 0.8367\n",
      "Epoch 160: Train Loss: 0.12029149383306503, Val Loss: 0.5698738098144531, Val Acc: 0.8163\n",
      "Epoch 180: Train Loss: 0.12677203118801117, Val Loss: 0.8192315101623535, Val Acc: 0.8367\n",
      "Fold 8/10\n",
      "Epoch 0: Train Loss: 1.9535776376724243, Val Loss: 1.9631236791610718, Val Acc: 0.6122\n",
      "Epoch 20: Train Loss: 0.2746940851211548, Val Loss: 1.1841422319412231, Val Acc: 0.7755\n",
      "Epoch 40: Train Loss: 0.20266121625900269, Val Loss: 1.5150141716003418, Val Acc: 0.7959\n",
      "Epoch 60: Train Loss: 0.17946964502334595, Val Loss: 1.237969994544983, Val Acc: 0.7551\n",
      "Epoch 80: Train Loss: 0.1769992709159851, Val Loss: 1.3549518585205078, Val Acc: 0.7959\n",
      "Epoch 100: Train Loss: 0.149765744805336, Val Loss: 1.0606563091278076, Val Acc: 0.7755\n",
      "Epoch 120: Train Loss: 0.14199039340019226, Val Loss: 1.1248269081115723, Val Acc: 0.7551\n",
      "Epoch 140: Train Loss: 0.1412438452243805, Val Loss: 2.085825204849243, Val Acc: 0.7755\n",
      "Epoch 160: Train Loss: 0.13703839480876923, Val Loss: 1.1263947486877441, Val Acc: 0.7551\n",
      "Epoch 180: Train Loss: 0.08751175552606583, Val Loss: 1.2168272733688354, Val Acc: 0.7347\n",
      "Fold 9/10\n",
      "Epoch 0: Train Loss: 1.9368504285812378, Val Loss: 1.9386223554611206, Val Acc: 0.5306\n",
      "Epoch 20: Train Loss: 0.27439334988594055, Val Loss: 0.8979426026344299, Val Acc: 0.7755\n",
      "Epoch 40: Train Loss: 0.23296447098255157, Val Loss: 0.8583254814147949, Val Acc: 0.8163\n",
      "Epoch 60: Train Loss: 0.20306606590747833, Val Loss: 0.8978810906410217, Val Acc: 0.8163\n",
      "Epoch 80: Train Loss: 0.17437006533145905, Val Loss: 0.7581672668457031, Val Acc: 0.8163\n",
      "Epoch 100: Train Loss: 0.139646515250206, Val Loss: 0.7574012875556946, Val Acc: 0.8367\n",
      "Epoch 120: Train Loss: 0.12596331536769867, Val Loss: 0.9095540046691895, Val Acc: 0.7755\n",
      "Epoch 140: Train Loss: 0.09157495945692062, Val Loss: 0.8124404549598694, Val Acc: 0.7959\n",
      "Epoch 160: Train Loss: 0.14399942755699158, Val Loss: 0.728417158126831, Val Acc: 0.7959\n",
      "Epoch 180: Train Loss: 0.1278143972158432, Val Loss: 0.8792576193809509, Val Acc: 0.7959\n",
      "Fold 10/10\n",
      "Epoch 0: Train Loss: 1.9415639638900757, Val Loss: 1.940518856048584, Val Acc: 0.5714\n",
      "Epoch 20: Train Loss: 0.30409231781959534, Val Loss: 0.7076156139373779, Val Acc: 0.8571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: Train Loss: 0.19972941279411316, Val Loss: 0.5517695546150208, Val Acc: 0.8776\n",
      "Epoch 60: Train Loss: 0.18421973288059235, Val Loss: 0.7581912279129028, Val Acc: 0.8776\n",
      "Epoch 80: Train Loss: 0.17658637464046478, Val Loss: 0.6309303641319275, Val Acc: 0.8776\n",
      "Epoch 100: Train Loss: 0.15827929973602295, Val Loss: 0.35029083490371704, Val Acc: 0.8776\n",
      "Epoch 120: Train Loss: 0.16494455933570862, Val Loss: 0.7896950244903564, Val Acc: 0.8571\n",
      "Epoch 140: Train Loss: 0.14260943233966827, Val Loss: 0.7064682841300964, Val Acc: 0.8776\n",
      "Epoch 160: Train Loss: 0.12550710141658783, Val Loss: 0.7748503684997559, Val Acc: 0.8571\n",
      "Epoch 180: Train Loss: 0.10190887749195099, Val Loss: 0.9430544376373291, Val Acc: 0.8367\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_hidden, num_classes, heads=12, output_heads=1):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(num_node_features, num_hidden, heads=heads, dropout=0.2)\n",
    "        self.conv2 = GATConv(num_hidden*heads, num_classes, heads=output_heads, concat=False, dropout=0.1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # First Graph Attention Layer\n",
    "        x = F.dropout(x,training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Second Graph Attention Layer\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "\n",
    "\n",
    "device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "k = 10\n",
    "\n",
    "kf = StratifiedKFold(n_splits=k)\n",
    "idx_train_np = np.array(idx_train)\n",
    "labels = data.y.numpy()[idx_train_np]\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(idx_train_np, labels)):\n",
    "    print(f\"Fold {fold+1}/{k}\")\n",
    "    model = GAT(num_node_features=data.x.shape[1], \n",
    "                num_hidden=128,\n",
    "                num_classes=(data.y.max()+1).item()\n",
    "               ).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.009, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    \n",
    "    data.train_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    data.val_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    data.train_mask[idx_train_np[train_idx]] = True\n",
    "    data.val_mask[idx_train_np[val_idx]] = True\n",
    "    \n",
    "    best_val_acc = 0 \n",
    "    best_model_state = None \n",
    "    \n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "#         loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask], ignore_index=-1)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                pred = model(data).argmax(dim=1)\n",
    "                correct = (pred[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "                acc = int(correct) / int(data.val_mask.sum())\n",
    "                \n",
    "                valid_labels_mask = data.y[data.val_mask] != -1  # Mask to select valid labels not equal to -1\n",
    "                correct_predictions = pred[data.val_mask][valid_labels_mask] == data.y[data.val_mask][valid_labels_mask]\n",
    "                correct = correct_predictions.sum()\n",
    "                acc = int(correct) / int(valid_labels_mask.sum())\n",
    "                \n",
    "                if acc > best_val_acc and acc >= 0.87:\n",
    "                    best_val_acc = acc\n",
    "                    best_model_state = model.state_dict()\n",
    "                    \n",
    "                val_loss = F.nll_loss(out[data.val_mask], data.y[data.val_mask], ignore_index=-1)\n",
    "                print(f'Epoch {epoch}: Train Loss: {loss.item()}, Val Loss: {val_loss.item()}, Val Acc: {acc:.4f}')\n",
    "            \n",
    "    if best_model_state is not None:\n",
    "        torch.save(best_model_state, f'gat_best_fold_{fold+1}.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSage Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/9\n",
      "Epoch 0: Train Loss: 1.9512313604354858, Val Loss: 1.9600183963775635, Val Acc: 0.4107\n",
      "Epoch 20: Train Loss: 0.062193144112825394, Val Loss: 0.4547552168369293, Val Acc: 0.8214\n",
      "Epoch 40: Train Loss: 0.008023635484278202, Val Loss: 0.5155643224716187, Val Acc: 0.8571\n",
      "Epoch 60: Train Loss: 0.0069757429882884026, Val Loss: 0.5265671610832214, Val Acc: 0.8571\n",
      "Epoch 80: Train Loss: 0.012904070317745209, Val Loss: 0.5834657549858093, Val Acc: 0.8393\n",
      "Epoch 100: Train Loss: 0.01250808872282505, Val Loss: 0.43230512738227844, Val Acc: 0.8393\n",
      "Epoch 120: Train Loss: 0.012719176709651947, Val Loss: 0.40123167634010315, Val Acc: 0.8214\n",
      "Epoch 140: Train Loss: 0.011782687157392502, Val Loss: 0.47539427876472473, Val Acc: 0.8393\n",
      "Epoch 160: Train Loss: 0.011525682173669338, Val Loss: 0.5100191235542297, Val Acc: 0.8571\n",
      "Epoch 180: Train Loss: 0.010711605660617352, Val Loss: 0.529053270816803, Val Acc: 0.8214\n",
      "Fold 2/9\n",
      "Epoch 0: Train Loss: 1.9714949131011963, Val Loss: 1.979977011680603, Val Acc: 0.5091\n",
      "Epoch 20: Train Loss: 0.06464030593633652, Val Loss: 0.7076171636581421, Val Acc: 0.7455\n",
      "Epoch 40: Train Loss: 0.008457829244434834, Val Loss: 1.0979831218719482, Val Acc: 0.7273\n",
      "Epoch 60: Train Loss: 0.007977909408509731, Val Loss: 1.078766107559204, Val Acc: 0.7091\n",
      "Epoch 80: Train Loss: 0.01152543444186449, Val Loss: 0.8021582961082458, Val Acc: 0.7273\n",
      "Epoch 100: Train Loss: 0.01191210187971592, Val Loss: 0.9344393014907837, Val Acc: 0.7455\n",
      "Epoch 120: Train Loss: 0.009803900495171547, Val Loss: 0.8998180627822876, Val Acc: 0.7091\n",
      "Epoch 140: Train Loss: 0.010987767018377781, Val Loss: 0.8931674361228943, Val Acc: 0.7455\n",
      "Epoch 160: Train Loss: 0.011650520376861095, Val Loss: 0.9940650463104248, Val Acc: 0.7273\n",
      "Epoch 180: Train Loss: 0.009876424446702003, Val Loss: 0.7915400266647339, Val Acc: 0.7273\n",
      "Fold 3/9\n",
      "Epoch 0: Train Loss: 1.948124647140503, Val Loss: 1.9450281858444214, Val Acc: 0.4727\n",
      "Epoch 20: Train Loss: 0.07934144139289856, Val Loss: 0.6143738031387329, Val Acc: 0.8182\n",
      "Epoch 40: Train Loss: 0.007597221527248621, Val Loss: 0.6814646124839783, Val Acc: 0.7818\n",
      "Epoch 60: Train Loss: 0.007530259434133768, Val Loss: 0.6952973008155823, Val Acc: 0.8000\n",
      "Epoch 80: Train Loss: 0.010821912437677383, Val Loss: 0.7741724252700806, Val Acc: 0.7818\n",
      "Epoch 100: Train Loss: 0.010709324851632118, Val Loss: 0.6719473600387573, Val Acc: 0.7818\n",
      "Epoch 120: Train Loss: 0.010348096489906311, Val Loss: 0.6269939541816711, Val Acc: 0.7636\n",
      "Epoch 140: Train Loss: 0.009866926819086075, Val Loss: 0.6798403859138489, Val Acc: 0.8000\n",
      "Epoch 160: Train Loss: 0.010321108624339104, Val Loss: 0.698113739490509, Val Acc: 0.8000\n",
      "Epoch 180: Train Loss: 0.009004342369735241, Val Loss: 0.5674072504043579, Val Acc: 0.7818\n",
      "Fold 4/9\n",
      "Epoch 0: Train Loss: 1.96453857421875, Val Loss: 1.9545246362686157, Val Acc: 0.4909\n",
      "Epoch 20: Train Loss: 0.05807850509881973, Val Loss: 0.6124264001846313, Val Acc: 0.8182\n",
      "Epoch 40: Train Loss: 0.007049964275211096, Val Loss: 0.7724571228027344, Val Acc: 0.8000\n",
      "Epoch 60: Train Loss: 0.007597367279231548, Val Loss: 0.720423698425293, Val Acc: 0.8000\n",
      "Epoch 80: Train Loss: 0.010129634290933609, Val Loss: 0.6329939961433411, Val Acc: 0.8000\n",
      "Epoch 100: Train Loss: 0.012922996655106544, Val Loss: 0.6251181960105896, Val Acc: 0.8182\n",
      "Epoch 120: Train Loss: 0.01058120746165514, Val Loss: 0.6534990668296814, Val Acc: 0.8182\n",
      "Epoch 140: Train Loss: 0.011237128637731075, Val Loss: 0.7435832619667053, Val Acc: 0.8182\n",
      "Epoch 160: Train Loss: 0.010979975573718548, Val Loss: 0.6620317697525024, Val Acc: 0.8364\n",
      "Epoch 180: Train Loss: 0.009420081041753292, Val Loss: 0.5176015496253967, Val Acc: 0.8182\n",
      "Fold 5/9\n",
      "Epoch 0: Train Loss: 1.9539250135421753, Val Loss: 1.9404544830322266, Val Acc: 0.4000\n",
      "Epoch 20: Train Loss: 0.06127134710550308, Val Loss: 0.5545399785041809, Val Acc: 0.8545\n",
      "Epoch 40: Train Loss: 0.006689780857414007, Val Loss: 0.5525063872337341, Val Acc: 0.8727\n",
      "Epoch 60: Train Loss: 0.007128944620490074, Val Loss: 0.6028433442115784, Val Acc: 0.8909\n",
      "Epoch 80: Train Loss: 0.010784022510051727, Val Loss: 0.6072447896003723, Val Acc: 0.9091\n",
      "Epoch 100: Train Loss: 0.010731441900134087, Val Loss: 0.6682204008102417, Val Acc: 0.8909\n",
      "Epoch 120: Train Loss: 0.009405902586877346, Val Loss: 0.6250190734863281, Val Acc: 0.8909\n",
      "Epoch 140: Train Loss: 0.010857995599508286, Val Loss: 0.6125490069389343, Val Acc: 0.8909\n",
      "Epoch 160: Train Loss: 0.008746699430048466, Val Loss: 0.6022437214851379, Val Acc: 0.9091\n",
      "Epoch 180: Train Loss: 0.009248285554349422, Val Loss: 0.6472944021224976, Val Acc: 0.8909\n",
      "Fold 6/9\n",
      "Epoch 0: Train Loss: 1.9299559593200684, Val Loss: 1.9264349937438965, Val Acc: 0.4364\n",
      "Epoch 20: Train Loss: 0.07028718292713165, Val Loss: 0.37426963448524475, Val Acc: 0.8909\n",
      "Epoch 40: Train Loss: 0.011283250525593758, Val Loss: 0.6337317228317261, Val Acc: 0.8545\n",
      "Epoch 60: Train Loss: 0.008223501965403557, Val Loss: 0.6498371958732605, Val Acc: 0.8727\n",
      "Epoch 80: Train Loss: 0.011507650837302208, Val Loss: 0.5209800004959106, Val Acc: 0.8727\n",
      "Epoch 100: Train Loss: 0.012454498559236526, Val Loss: 0.4970857501029968, Val Acc: 0.8727\n",
      "Epoch 120: Train Loss: 0.01215396262705326, Val Loss: 0.5136942267417908, Val Acc: 0.8727\n",
      "Epoch 140: Train Loss: 0.011447031982243061, Val Loss: 0.589708685874939, Val Acc: 0.9091\n",
      "Epoch 160: Train Loss: 0.009923689067363739, Val Loss: 0.5580247044563293, Val Acc: 0.8909\n",
      "Epoch 180: Train Loss: 0.008645066991448402, Val Loss: 0.47902676463127136, Val Acc: 0.8727\n",
      "Fold 7/9\n",
      "Epoch 0: Train Loss: 1.927802324295044, Val Loss: 1.9642102718353271, Val Acc: 0.5636\n",
      "Epoch 20: Train Loss: 0.05317283421754837, Val Loss: 1.080920696258545, Val Acc: 0.8182\n",
      "Epoch 40: Train Loss: 0.006097463890910149, Val Loss: 1.3738465309143066, Val Acc: 0.8000\n",
      "Epoch 60: Train Loss: 0.006999720353633165, Val Loss: 1.3381458520889282, Val Acc: 0.8182\n",
      "Epoch 80: Train Loss: 0.010009963065385818, Val Loss: 1.357527494430542, Val Acc: 0.8182\n",
      "Epoch 100: Train Loss: 0.010737836360931396, Val Loss: 1.2247560024261475, Val Acc: 0.8000\n",
      "Epoch 120: Train Loss: 0.009983276017010212, Val Loss: 1.1322624683380127, Val Acc: 0.8000\n",
      "Epoch 140: Train Loss: 0.009432343766093254, Val Loss: 1.2992572784423828, Val Acc: 0.8000\n",
      "Epoch 160: Train Loss: 0.008735034614801407, Val Loss: 1.092769980430603, Val Acc: 0.8000\n",
      "Epoch 180: Train Loss: 0.008330016396939754, Val Loss: 1.350192666053772, Val Acc: 0.8000\n",
      "Fold 8/9\n",
      "Epoch 0: Train Loss: 1.9185303449630737, Val Loss: 1.9406147003173828, Val Acc: 0.4000\n",
      "Epoch 20: Train Loss: 0.06012866646051407, Val Loss: 0.4810255169868469, Val Acc: 0.8727\n",
      "Epoch 40: Train Loss: 0.007025735452771187, Val Loss: 0.6124266386032104, Val Acc: 0.8727\n",
      "Epoch 60: Train Loss: 0.005914529785513878, Val Loss: 0.6885313987731934, Val Acc: 0.8727\n",
      "Epoch 80: Train Loss: 0.010519275441765785, Val Loss: 0.5807756185531616, Val Acc: 0.8727\n",
      "Epoch 100: Train Loss: 0.011390559375286102, Val Loss: 0.570447564125061, Val Acc: 0.8727\n",
      "Epoch 120: Train Loss: 0.01073350291699171, Val Loss: 0.5391541123390198, Val Acc: 0.8545\n",
      "Epoch 140: Train Loss: 0.011203295551240444, Val Loss: 0.6350927948951721, Val Acc: 0.8545\n",
      "Epoch 160: Train Loss: 0.010364511050283909, Val Loss: 0.6821459531784058, Val Acc: 0.8545\n",
      "Epoch 180: Train Loss: 0.009423750452697277, Val Loss: 0.6457028388977051, Val Acc: 0.8727\n",
      "Fold 9/9\n",
      "Epoch 0: Train Loss: 1.9356902837753296, Val Loss: 1.9492292404174805, Val Acc: 0.4727\n",
      "Epoch 20: Train Loss: 0.07201319187879562, Val Loss: 0.6032397747039795, Val Acc: 0.8364\n",
      "Epoch 40: Train Loss: 0.007348522078245878, Val Loss: 0.6813511252403259, Val Acc: 0.8364\n",
      "Epoch 60: Train Loss: 0.007062521763145924, Val Loss: 0.8109342455863953, Val Acc: 0.8364\n",
      "Epoch 80: Train Loss: 0.011917189694941044, Val Loss: 0.5810433626174927, Val Acc: 0.8364\n",
      "Epoch 100: Train Loss: 0.012834168039262295, Val Loss: 0.5718836188316345, Val Acc: 0.8364\n",
      "Epoch 120: Train Loss: 0.011104434728622437, Val Loss: 0.5924055576324463, Val Acc: 0.8364\n",
      "Epoch 140: Train Loss: 0.012662907131016254, Val Loss: 0.647836446762085, Val Acc: 0.8182\n",
      "Epoch 160: Train Loss: 0.009358860552310944, Val Loss: 0.6857406497001648, Val Acc: 0.8364\n",
      "Epoch 180: Train Loss: 0.007711118087172508, Val Loss: 0.5780282020568848, Val Acc: 0.8364\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_hidden, num_classes):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(num_node_features, num_hidden)\n",
    "        self.conv2 = SAGEConv(num_hidden, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # First GraphSAGE Layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        # Second GraphSAGE Layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    \n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "k = 9\n",
    "kf = StratifiedKFold(n_splits=k)\n",
    "idx_train_np = np.array(idx_train)\n",
    "labels = data.y.numpy()[idx_train_np]\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(idx_train_np, labels)):\n",
    "    print(f\"Fold {fold+1}/{k}\")\n",
    "    model = GraphSAGE(num_node_features=data.x.shape[1], \n",
    "                num_hidden=128,\n",
    "                num_classes=(data.y.max()+1).item()\n",
    "               ).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.009, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    \n",
    "    data.train_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    data.val_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    data.train_mask[idx_train_np[train_idx]] = True\n",
    "    data.val_mask[idx_train_np[val_idx]] = True\n",
    "    \n",
    "    best_val_acc = 0 \n",
    "    best_model_state = None \n",
    "    \n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "#         loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask], ignore_index=-1)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                pred = model(data).argmax(dim=1)\n",
    "                correct = (pred[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "                acc = int(correct) / int(data.val_mask.sum())\n",
    "                \n",
    "                valid_labels_mask = data.y[data.val_mask] != -1  # Mask to select valid labels not equal to -1\n",
    "                correct_predictions = pred[data.val_mask][valid_labels_mask] == data.y[data.val_mask][valid_labels_mask]\n",
    "                correct = correct_predictions.sum()\n",
    "                acc = int(correct) / int(valid_labels_mask.sum())\n",
    "                \n",
    "                if acc > best_val_acc and acc >= 0.85:\n",
    "                    best_val_acc = acc\n",
    "                    best_model_state = model.state_dict()\n",
    "                    \n",
    "                val_loss = F.nll_loss(out[data.val_mask], data.y[data.val_mask], ignore_index=-1)\n",
    "                print(f'Epoch {epoch}: Train Loss: {loss.item()}, Val Loss: {val_loss.item()}, Val Acc: {acc:.4f}')\n",
    "            \n",
    "    if best_model_state is not None:\n",
    "        torch.save(best_model_state, f'gsage_best_fold_{fold+1}.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking GCN, GAT & GSage Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rp/p6mr7_3s7xq258wywpvvwhn40000gn/T/ipykernel_44836/4148480833.py:51: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  majority_votes, _ = mode(all_predictions, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from scipy.stats import mode\n",
    "\n",
    "device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = (data.y.max() + 1).item() \n",
    "\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "model_types = ['gcn', 'gat', 'gsage','appnp'] \n",
    "num_folds = 10\n",
    "\n",
    "for model_type in model_types:\n",
    "    for fold in range(1, num_folds + 1):\n",
    "        if model_type == 'gcn':\n",
    "            model = GCN(num_node_features=data.x.shape[1], \n",
    "                num_hidden=64,\n",
    "                num_classes=(data.y.max()+1).item()\n",
    "               ).to(device)\n",
    "        elif model_type == 'gat':\n",
    "            model = GAT(num_node_features=data.x.shape[1], \n",
    "                num_hidden=128,\n",
    "                num_classes=(data.y.max()+1).item()\n",
    "               ).to(device)\n",
    "        \n",
    "        elif model_type == 'appnp':\n",
    "            model = APPNPNet(num_node_features=data.x.shape[1], \n",
    "                     num_hidden=64,\n",
    "                     num_classes=(data.y.max()+1).item(),\n",
    "                     K=10,\n",
    "                     alpha=0.1).to(device)\n",
    "        elif model_type == 'gsage':\n",
    "            model = GraphSAGE(num_node_features=data.x.shape[1], \n",
    "                num_hidden=128,\n",
    "                num_classes=(data.y.max()+1).item()\n",
    "               ).to(device)\n",
    "            \n",
    "        try:\n",
    "            model_path = f'./{model_type}_best_fold_{fold}.pt' \n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = model(data.to(device))\n",
    "                preds = out.argmax(dim=1)\n",
    "                all_predictions.append(preds.cpu().numpy())\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "majority_votes, _ = mode(all_predictions, axis=0)\n",
    "majority_votes = torch.tensor(majority_votes.squeeze(), dtype=torch.long)\n",
    "# correct = (majority_votes[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "# acc = int(correct) / int(data.val_mask.sum())\n",
    "# print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# pred = model(data).argmax(dim=1)\n",
    "# correct = (pred[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "# acc = int(correct) / int(data.val_mask.sum())\n",
    "# print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submitting the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = majority_votes[idx_test]\n",
    "np.savetxt('submission.txt', preds, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
